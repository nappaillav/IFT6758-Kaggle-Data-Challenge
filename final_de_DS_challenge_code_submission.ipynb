{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "final de DS_challenge_code_submission.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "xs8KZf29sq-6",
        "QFEMrOh-_DtN"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vincentcommere/IFT6758-Kaggle-Data-Challenge/blob/main/final_de_DS_challenge_code_submission.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fehU89pVkiE4"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4FHEMroIkjCh"
      },
      "source": [
        "# README :\n",
        "\n",
        "- Open in Colab *(Html titles wont appears in the Markdown reader of GitHub)*\n",
        "- Update your project path drive in the **Environment Set-up** section\n",
        "- Run all cells of **Usefull functions** section\n",
        "- Run all cells of **Data cleaning & Feature engineering Classes** section\n",
        "- Run all cells of **Model - 1** section\n",
        "- You will get the **curr_submission.csv** submission file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FKe9Phi4zidN"
      },
      "source": [
        "#**Environment Set-up**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rOkaKAc_Zvnr",
        "outputId": "6187691d-9902-4271-8cc3-aa69dab0aef3"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TGO3lb4sPID6"
      },
      "source": [
        "<font color='red'>**Feel free to change the following path base on your own Drive**</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SjBMOpdcZAtu"
      },
      "source": [
        "cp -R /content/drive/MyDrive/02_ETUDES/01-MILA-UDEM/IFT6758/Kaggle/* ."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I2eqsV0zZAt0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7aa54bd-f11c-4b51-ca32-1e267adce6ec"
      },
      "source": [
        "!unzip -qq ift6758-a20.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "replace sample_submission.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iILtL-g7YBNE"
      },
      "source": [
        "### _Librairies Installations_"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qtZzIpVMSZld",
        "outputId": "73b87bd4-b4ef-4ddf-a15a-fe5c9c9e81a6"
      },
      "source": [
        "!pip install catboost\n",
        "!pip install lightgbm\n",
        "!pip install scikit-image\n",
        "!pip install opencv-python"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: catboost in /usr/local/lib/python3.6/dist-packages (0.24.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from catboost) (1.4.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from catboost) (1.15.0)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.6/dist-packages (from catboost) (4.4.1)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from catboost) (1.19.4)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.6/dist-packages (from catboost) (1.1.5)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.6/dist-packages (from catboost) (0.10.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from catboost) (3.2.2)\n",
            "Requirement already satisfied: retrying>=1.3.3 in /usr/local/lib/python3.6/dist-packages (from plotly->catboost) (1.3.3)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.24.0->catboost) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.24.0->catboost) (2.8.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->catboost) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->catboost) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->catboost) (1.3.1)\n",
            "Requirement already satisfied: lightgbm in /usr/local/lib/python3.6/dist-packages (2.2.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from lightgbm) (1.4.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from lightgbm) (1.19.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from lightgbm) (0.22.2.post1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->lightgbm) (1.0.0)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.6/dist-packages (0.16.2)\n",
            "Requirement already satisfied: pillow>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image) (7.0.0)\n",
            "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image) (3.2.2)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image) (2.4.1)\n",
            "Requirement already satisfied: scipy>=0.19.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image) (1.4.1)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image) (2.5)\n",
            "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image) (1.1.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image) (1.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image) (2.8.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image) (2.4.7)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image) (1.19.4)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx>=2.0->scikit-image) (4.4.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from cycler>=0.10->matplotlib!=3.0.0,>=2.0.0->scikit-image) (1.15.0)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.6/dist-packages (4.1.2.30)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from opencv-python) (1.19.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aDqjH4Q3X-Ix"
      },
      "source": [
        "### _Globales Variables_"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j7gj5fIWSPjz"
      },
      "source": [
        "train_csv_path = 'train.csv'\n",
        "test_csv_path = 'test.csv'\n",
        "cluster_csv_path = 'cluster_features.npy'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CNQ2PT75O6c2"
      },
      "source": [
        "## *Libraries Imports*\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XvoMZs6oRWMR"
      },
      "source": [
        "import csv\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn import preprocessing\n",
        "from sklearn.preprocessing import PowerTransformer\n",
        "from sklearn import linear_model\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import (\n",
        "    RandomForestRegressor, GradientBoostingRegressor, \n",
        "    StackingRegressor, BaggingRegressor,\n",
        ")\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.model_selection import KFold, cross_val_predict\n",
        "from sklearn.metrics import mean_squared_log_error\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xRGdV0Kcezvu"
      },
      "source": [
        "# <font color=''>**Usefull functions**</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gn-xkaleVFVT"
      },
      "source": [
        "### *Evaluation metric function*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Wz90Xi353WT"
      },
      "source": [
        "def rmsle(y_true, y_pred):\n",
        "    \"\"\"Calculate RMSLE for predictions\"\"\"\n",
        "    return np.sqrt(mean_squared_log_error(y_true, y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0DYAlD8DVLrk"
      },
      "source": [
        "### *Compute cross validation function*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ssml-JlXVOnR"
      },
      "source": [
        "def get_cv_predictions(model, X_train, y_train, n_splits=5, shuffle=True):\n",
        "    cv_fold = KFold(n_splits=5, random_state=42, shuffle=True)\n",
        "    return cross_val_predict(model, X_train, y_train, cv=cv_fold)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5JHGtJEeK-E"
      },
      "source": [
        "# <font color=''>**Data cleaning & Feature engineering Classes**</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SmgeZui1Ui5w"
      },
      "source": [
        "### _Imputer Class_"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0yLX6sg35nez"
      },
      "source": [
        "class Imputer:\n",
        "    @staticmethod\n",
        "    def _fillna(df, col_name, value, na_symbol=None):\n",
        "        if na_symbol:\n",
        "            df[col_name].replace(na_symbol, np.NaN, inplace=True)\n",
        "        df[col_name].fillna(value, inplace=True)\n",
        "\n",
        "    def impute_data(self, df, train):\n",
        "        if train:\n",
        "            df.dropna(subset=['num_of_profile_likes'], axis=0, inplace=True)\n",
        "        self._fillna(df, col_name='profile_cover_image_status', value='Not set')\n",
        "        self._fillna(df, col_name='is_profile_view_size_customized?', value=False)\n",
        "        self._fillna(df, col_name='location_public_visibility', value='disabled', na_symbol='??')\n",
        "        self._fillna(df, col_name='profile_category', value='unknown', na_symbol=' ')\n",
        "        self._fillna(df, col_name='avg_daily_profile_visit_duration_in_seconds', value=df['avg_daily_profile_visit_duration_in_seconds'].median())\n",
        "        self._fillna(df, col_name='avg_daily_profile_clicks', value=df['avg_daily_profile_clicks'].median())\n",
        "\n",
        "        self._fillna(df, col_name='utc_offset', value=df.groupby(['utc_offset', 'location'])['utc_offset'].transform('max'))\n",
        "        self._fillna(df, col_name='utc_offset', value=0)\n",
        "        return df\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vxngn2SxUna5"
      },
      "source": [
        "### _Feature Engineering Class_"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "acdQm1eK5rTq"
      },
      "source": [
        "class FeatureEngineer:\n",
        "    scaler = None\n",
        "\n",
        "    def __init__(self, scaler=None):\n",
        "        self.scaler = scaler or preprocessing.StandardScaler()\n",
        "\n",
        "    def process_data(self, df, train):\n",
        "      \n",
        "        df['has_personal_url'] = df['personal_url'].notna().astype(int)\n",
        "        df['cover_image_set'] = df['profile_cover_image_status'].apply(lambda x: 1 if x == 'Set' else 0)\n",
        "        df['view_size_customized'] = df['is_profile_view_size_customized?'].astype(int)\n",
        "        df['is_location_visible'] = df['location_public_visibility'].apply(lambda x: 1 if x.lower() == 'enabled' else 0)\n",
        "\n",
        "        df = df.join(self._encode_category(df, 'profile_verification_status', prefix='status'))\n",
        "        df = df.join(self._encode_category(df, 'profile_category', prefix='category'))\n",
        "        df = df.join(self._encode_category(df, 'user_language', prefix='lang'))\n",
        "        df['creation_year'] = df['profile_creation_timestamp'].apply(lambda x: x.year)\n",
        "        df['creation_year'] = df['creation_year'] - df['creation_year'].min()\n",
        "\n",
        "        df = self._normalize_columns(df, col_names=[\n",
        "            'utc_offset', 'num_of_followers', 'num_of_people_following',\n",
        "            'num_of_status_updates', 'num_of_direct_messages',\n",
        "            'avg_daily_profile_visit_duration_in_seconds', 'avg_daily_profile_clicks',\n",
        "        ], train=train)\n",
        "\n",
        "        self._drop_columns(df, col_names=[\n",
        "            'id', 'user_name', 'personal_url', 'profile_cover_image_status', \n",
        "            'is_profile_view_size_customized?', 'location_public_visibility', \n",
        "            'profile_verification_status', 'profile_text_color', 'profile_page_color', \n",
        "            'profile_theme_color',  'location', 'user_time_zone', 'profile_category', \n",
        "            'profile_image', 'profile_creation_timestamp', 'user_language',\n",
        "        ])\n",
        "        return df\n",
        "\n",
        "    def _normalize_columns(self, df, col_names, train):\n",
        "        if train:\n",
        "            self.scaler.fit(df[col_names].values)\n",
        "        df.update(pd.DataFrame(self.scaler.transform(df[col_names].values), columns=col_names))\n",
        "        return df\n",
        "\n",
        "    @staticmethod\n",
        "    def _encode_category(df, col_name, target_encoding=False, prefix='profile'):\n",
        "        df[col_name] = df[col_name].apply(lambda x: x.replace(' ', '_').lower())\n",
        "        # TODO: change to sklearn encoders\n",
        "        return pd.get_dummies(df[col_name], prefix=prefix)\n",
        "\n",
        "    @staticmethod\n",
        "    def _drop_columns(df, col_names):\n",
        "        df.drop(col_names, axis=1, inplace=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhcgIUuRUwOs"
      },
      "source": [
        "### _Data Processing Class_"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nYC3EgRG5zv2"
      },
      "source": [
        "class DataPreprocessor:\n",
        "    imputer = None\n",
        "    feature_eng = None\n",
        "\n",
        "    def __init__(self, imputer=None, feature_eng=None):\n",
        "        self.imputer = imputer or Imputer()\n",
        "        self.feature_eng = feature_eng or FeatureEngineer()\n",
        "\n",
        "    def preprocess(self, df, train=False):\n",
        "        df = self._process_column_names(df)\n",
        "        df = self._handle_missing_values(df, train=train)\n",
        "        df = self._engineer_features(df, train=train)\n",
        "        return df\n",
        "\n",
        "    @staticmethod\n",
        "    def _process_column_names(df):\n",
        "        \"\"\"Convert column names to follow variables notation of python for convenience\"\"\"\n",
        "        return df.rename(columns=lambda x: x.replace(' ', '_').lower())\n",
        "\n",
        "    def _handle_missing_values(self, df, train):\n",
        "        return self.imputer.impute_data(df, train=train)\n",
        "\n",
        "    def _engineer_features(self, df, train):\n",
        "        return self.feature_eng.process_data(df, train=train)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5tE54YEiVcSo"
      },
      "source": [
        "# <font color=''>**Data Preparation 1**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EHhLMcID1q6i"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "  \n",
        "\n",
        "    numeric_cols = [\n",
        "    'utc_offset', 'num_of_followers', 'num_of_people_following', 'num_of_status_updates', \n",
        "    'num_of_direct_messages', 'avg_daily_profile_visit_duration_in_seconds', \n",
        "    'avg_daily_profile_clicks'\n",
        "    ]\n",
        "\n",
        "    df_train = pd.read_csv(train_csv_path, parse_dates=['Profile Creation Timestamp'])\n",
        "    df_test = pd.read_csv(test_csv_path, parse_dates=['Profile Creation Timestamp'])\n",
        "    test_id = df_test['Id'].copy()\n",
        "\n",
        "    preprocessor = DataPreprocessor()\n",
        "    df_train = preprocessor.preprocess(df_train, train=True)\n",
        "\n",
        "    ### ----- PARTS TO ADD INSIDE PREPROCESSOR --------\n",
        "    pow_transform = PowerTransformer()\n",
        "    df_train[numeric_cols] = pow_transform.fit_transform(df_train[numeric_cols])\n",
        "\n",
        "    df_train['num_of_profile_likes'] = df_train['num_of_profile_likes'].apply(np.log1p)\n",
        "    X_train, y_train = df_train.loc[:, df_train.columns != 'num_of_profile_likes'], df_train['num_of_profile_likes']\n",
        "\n",
        "    df_test = preprocessor.preprocess(df_test, train=False)\n",
        "    # TODO: Need to add a label encoder for language\n",
        "    # instead of pd.dummies() as done in current preprocessing\n",
        "    df_test['lang_el'] = 0\n",
        "    df_test['lang_sk'] = 0\n",
        "    df_test['lang_uk'] = 0\n",
        "    df_test['lang_sr'] = 0\n",
        "    df_test['lang_zh-tw'] = 0\n",
        "    df_test['lang_da'] = 0\n",
        "    df_test.drop(columns=['lang_he', 'lang_no', 'lang_ro'], inplace=True)\n",
        "    df_test[numeric_cols] = pow_transform.transform(df_test[numeric_cols])\n",
        "    df_test = df_test[X_train.columns]\n",
        "    X_test = df_test\n",
        "\n",
        "    feat = np.load(cluster_csv_path, allow_pickle=True).tolist()\n",
        "    res = pd.get_dummies(feat[6]['train_features'], prefix='cluster')\n",
        "    for col in res.columns:\n",
        "        X_train[col] = res[col]\n",
        "    res = pd.get_dummies(feat[6]['test_features'], prefix='cluster')\n",
        "    for col in res.columns:\n",
        "        X_test[col] = res[col]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xs8KZf29sq-6"
      },
      "source": [
        "# <font color=''>**Model - 1 : Ensemble Method with clustering**\n",
        "\n",
        "- Submission #1 (best submitted private result on Kaggle)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qgl9WTEzUU9X"
      },
      "source": [
        "## _Models Parameters Definition_"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "io96coSXTE2M"
      },
      "source": [
        "params = {\n",
        "  'rf_reg': {\n",
        "  'n_estimators': 700,\n",
        "  'max_depth': 6,\n",
        "  'min_samples_split': 5,\n",
        "  'min_samples_leaf': 5,\n",
        "  'max_features': None,\n",
        "  'oob_score': True,\n",
        "  'random_state': 42\n",
        "  },\n",
        "  'svr': {\n",
        "  'C': 20, \n",
        "  'epsilon': 0.008, \n",
        "  'gamma': 0.0003\n",
        "  },\n",
        "  'bagging_reg': {\n",
        "  'n_estimators': 500, \n",
        "  'max_samples': 0.7, \n",
        "  'oob_score': True\n",
        "  },\n",
        "  'gb_reg': {\n",
        "  'n_estimators': 700,\n",
        "  'learning_rate': 0.01,\n",
        "  'max_depth': 7,\n",
        "  'max_features': 'sqrt',\n",
        "  'min_samples_leaf': 5,\n",
        "  'min_samples_split': 10,\n",
        "  'loss': 'huber',\n",
        "  'random_state': 42\n",
        "  },\n",
        "  'xgb_reg': {\n",
        "  'learning_rate': 0.01,\n",
        "  'n_estimators': 1000,\n",
        "  'max_depth': 5,\n",
        "  'gamma': 0.6,\n",
        "  'subsample': 0.8,\n",
        "  'colsample_bytree': 0.8,\n",
        "  'objective': 'reg:squarederror',\n",
        "  'nthread': -1,\n",
        "  'scale_pos_weight': 1,\n",
        "  'seed': 27,\n",
        "  'reg_alpha': 0.00006,\n",
        "  'random_state': 42\n",
        "  },\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3hGiil0NTjcS"
      },
      "source": [
        "### _Models definitions_"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CsXAcQPRTe4B"
      },
      "source": [
        "m_rf = RandomForestRegressor(**params['rf_reg'])\n",
        "m_svr = SVR(**params['svr'])\n",
        "m_bag = BaggingRegressor(**params['bagging_reg'])\n",
        "m_gbr = GradientBoostingRegressor(**params['gb_reg'])\n",
        "m_xgbr = XGBRegressor(**params['xgb_reg'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BrvkHr-tTpdO"
      },
      "source": [
        "### _Models Stacking_"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "57k0mOvkTf8H"
      },
      "source": [
        "m_ensemble = StackingRegressor(\n",
        "  estimators=[\n",
        "    ('random_forest', m_rf),\n",
        "    ('SVR', m_svr),\n",
        "    ('bagging', m_bag),\n",
        "    ('grad_boosting', m_gbr),\n",
        "    ('xgboost', m_xgbr),\n",
        "  ],\n",
        "  n_jobs=-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KY1vUS6eYiG6"
      },
      "source": [
        "### _Models Testing_"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g9PiiLb6RypN",
        "outputId": "f546932a-1f4f-4d03-85f8-9a0e8801d44f"
      },
      "source": [
        "cv_prediction = get_cv_predictions(m_ensemble, X_train, y_train, n_splits=5, shuffle=True)\n",
        "cv_prediction = np.abs(cv_prediction)\n",
        "print(rmsle(np.expm1(y_train), np.expm1(cv_prediction)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/joblib/externals/loky/process_executor.py:691: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n",
            "/usr/local/lib/python3.6/dist-packages/joblib/externals/loky/process_executor.py:691: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n",
            "/usr/local/lib/python3.6/dist-packages/joblib/externals/loky/process_executor.py:691: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n",
            "/usr/local/lib/python3.6/dist-packages/joblib/externals/loky/process_executor.py:691: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n",
            "/usr/local/lib/python3.6/dist-packages/joblib/externals/loky/process_executor.py:691: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1.7058835584148224\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBd_owtbT1-8"
      },
      "source": [
        "### _Stacking Model - Fit & Predict_"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tH8CfaXvR8Y8"
      },
      "source": [
        "m_ensemble.fit(X_train, y_train)\n",
        "pred = m_ensemble.predict(X_test)\n",
        "pred = np.abs(pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lP2hr4BBT_qJ"
      },
      "source": [
        "### *Submission file - 1*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJHSqSTkUCGJ"
      },
      "source": [
        "# pred.to_csv('curr_submission.csv',['Id', 'Predicted'])\n",
        "with open('curr_submission.csv', 'w') as csvfile:\n",
        "    csvwriter = csv.writer(csvfile)  \n",
        "    csvwriter.writerow(['Id', 'Predicted'])\n",
        "    for i in range(len(pred)):\n",
        "        csvwriter.writerow([test_id.iloc[i], np.expm1(pred[i])])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFEMrOh-_DtN"
      },
      "source": [
        "# <font color=''>**Data Preparation 2**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3pGoE6N3_DGr"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "  \n",
        "\n",
        "    numeric_cols = [\n",
        "    'utc_offset', 'num_of_followers', 'num_of_people_following', 'num_of_status_updates', \n",
        "    'num_of_direct_messages', 'avg_daily_profile_visit_duration_in_seconds', \n",
        "    'avg_daily_profile_clicks'\n",
        "    ]\n",
        "\n",
        "    df_train = pd.read_csv(train_csv_path, parse_dates=['Profile Creation Timestamp'])\n",
        "    df_test = pd.read_csv(test_csv_path, parse_dates=['Profile Creation Timestamp'])\n",
        "    test_id = df_test['Id'].copy()\n",
        "\n",
        "    preprocessor = DataPreprocessor()\n",
        "    df_train = preprocessor.preprocess(df_train, train=True)\n",
        "\n",
        "    ### ----- PARTS TO ADD INSIDE PREPROCESSOR --------\n",
        "    pow_transform = PowerTransformer()\n",
        "    df_train[numeric_cols] = pow_transform.fit_transform(df_train[numeric_cols])\n",
        "\n",
        "    df_train['num_of_profile_likes'] = df_train['num_of_profile_likes'].apply(np.log1p)\n",
        "    X_train, y_train = df_train.loc[:, df_train.columns != 'num_of_profile_likes'], df_train['num_of_profile_likes']\n",
        "\n",
        "    df_test = preprocessor.preprocess(df_test, train=False)\n",
        "    # TODO: Need to add a label encoder for language\n",
        "    # instead of pd.dummies() as done in current preprocessing\n",
        "    df_test['lang_el'] = 0\n",
        "    df_test['lang_sk'] = 0\n",
        "    df_test['lang_uk'] = 0\n",
        "    df_test['lang_sr'] = 0\n",
        "    df_test['lang_zh-tw'] = 0\n",
        "    df_test['lang_da'] = 0\n",
        "    df_test.drop(columns=['lang_he', 'lang_no', 'lang_ro'], inplace=True)\n",
        "    df_test[numeric_cols] = pow_transform.transform(df_test[numeric_cols])\n",
        "    df_test = df_test[X_train.columns]\n",
        "    X_test = df_test\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMVqYgUGf_pt"
      },
      "source": [
        "\n",
        "# **Model - 2 : Ensemble Method only**\n",
        "\n",
        "\n",
        "- Submission #2 (2nd best submitted private result on Kaggle)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1KgWnVOZJJmS"
      },
      "source": [
        "### _Models Parameters Definition_"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V4YBVdBrBIBm"
      },
      "source": [
        "params = {\n",
        "        'rf_reg': {\n",
        "            'n_estimators': 700,\n",
        "            'max_depth': 6,\n",
        "            'min_samples_split': 5,\n",
        "            'min_samples_leaf': 5,\n",
        "            'max_features': None,\n",
        "            'oob_score': True,\n",
        "            'random_state': 42\n",
        "        },\n",
        "        'svr': {\n",
        "            'C': 20, \n",
        "            'epsilon': 0.008, \n",
        "            'gamma': 0.0003\n",
        "        },\n",
        "        'bagging_reg': {\n",
        "            'n_estimators': 500, \n",
        "            'max_samples': 0.7, \n",
        "            'oob_score': True\n",
        "        },\n",
        "        'gb_reg': {\n",
        "            'n_estimators': 700,\n",
        "            'learning_rate': 0.01,\n",
        "            'max_depth': 7,\n",
        "            'max_features': 'sqrt',\n",
        "            'min_samples_leaf': 5,\n",
        "            'min_samples_split': 10,\n",
        "            'loss': 'huber',\n",
        "            'random_state': 42\n",
        "        },\n",
        "        'xgb_reg': {\n",
        "            'learning_rate': 0.01,\n",
        "            'n_estimators': 1000,\n",
        "            'max_depth': 5,\n",
        "            'gamma': 0.6,\n",
        "            'subsample': 0.8,\n",
        "            'colsample_bytree': 0.8,\n",
        "            'objective': 'reg:squarederror',\n",
        "            'nthread': -1,\n",
        "            'scale_pos_weight': 1,\n",
        "            'seed': 27,\n",
        "            'reg_alpha': 0.00006,\n",
        "            'random_state': 42\n",
        "        },\n",
        "    }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bz0Usau9JJmT"
      },
      "source": [
        "### _Models definitions_"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m3HlyAovJNIn"
      },
      "source": [
        "m_rf = RandomForestRegressor(**params['rf_reg'])\n",
        "m_svr = SVR(**params['svr'])\n",
        "m_bag = BaggingRegressor(**params['bagging_reg'])\n",
        "m_gbr = GradientBoostingRegressor(**params['gb_reg'])\n",
        "m_xgbr = XGBRegressor(**params['xgb_reg'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQ9qy3XlJJmU"
      },
      "source": [
        "### _Models Stacking_"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Vso6kOMJPCU"
      },
      "source": [
        "m_ensemble = StackingRegressor(\n",
        "    estimators=[\n",
        "    ('random_forest', m_rf),\n",
        "    ('SVR', m_svr),\n",
        "    ('bagging', m_bag),\n",
        "    ('grad_boosting', m_gbr),\n",
        "    ('xgboost', m_xgbr),\n",
        "  ],\n",
        "  n_jobs=-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5mxcxMgqJJmU"
      },
      "source": [
        "### _Models Testing_"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RU-OPHPI0QWR",
        "outputId": "b05a837c-6426-43ec-ee62-63e2d057bb8e"
      },
      "source": [
        "cv_prediction = get_cv_predictions(m_ensemble, X_train, y_train, n_splits=5, shuffle=True)\n",
        "cv_prediction = np.abs(cv_prediction)\n",
        "print(rmsle(np.expm1(y_train), np.expm1(cv_prediction)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/joblib/externals/loky/process_executor.py:691: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n",
            "/usr/local/lib/python3.6/dist-packages/joblib/externals/loky/process_executor.py:691: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n",
            "/usr/local/lib/python3.6/dist-packages/joblib/externals/loky/process_executor.py:691: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n",
            "/usr/local/lib/python3.6/dist-packages/joblib/externals/loky/process_executor.py:691: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n",
            "/usr/local/lib/python3.6/dist-packages/joblib/externals/loky/process_executor.py:691: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1.7059227078154642\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UcN80cgjJJmV"
      },
      "source": [
        "### _Stacking Model - Fit & Predict_"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0HnhP7UZ1RYY"
      },
      "source": [
        "m_ensemble.fit(X_train, y_train)\n",
        "pred = m_ensemble.predict(X_test)\n",
        "pred = np.abs(pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0WmF_cU1iwl"
      },
      "source": [
        "## *Submission file 2*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I1GI6vx7zOwx"
      },
      "source": [
        "with open('curr_submission2.csv', 'w') as csvfile:\n",
        "    csvwriter = csv.writer(csvfile)  \n",
        "    csvwriter.writerow(['Id', 'Predicted'])\n",
        "\n",
        "    for i in range(len(pred)):\n",
        "        csvwriter.writerow([test_id.iloc[i], np.expm1(pred[i])])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OXwMDXrQI7RE"
      },
      "source": [
        "\n",
        "# **Model - 3**\n",
        "\n",
        "\n",
        "- Submission #3 (3rd best submitted private result on Kaggle)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YSekwF4aA2-_"
      },
      "source": [
        "import csv\n",
        "import math\n",
        "import pdb\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import mean_squared_log_error\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import (\n",
        "    RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor, \n",
        "    VotingRegressor, StackingRegressor, BaggingRegressor,\n",
        ")\n",
        "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
        "from sklearn.metrics import make_scorer\n",
        "from sklearn import preprocessing\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.svm import SVR, LinearSVR\n",
        "from xgboost import XGBRegressor\n",
        "import seaborn as sns\n",
        "from  matplotlib import pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn import linear_model\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import RobustScaler, MinMaxScaler\n",
        "from scipy.special import boxcox1p, inv_boxcox1p\n",
        "from scipy.stats import boxcox_normmax\n",
        "from scipy.stats import skew, norm\n",
        "from scipy.stats import yeojohnson\n",
        "from sklearn.preprocessing import PowerTransformer\n",
        "from lightgbm import LGBMRegressor\n",
        "from catboost import CatBoostRegressor\n",
        "\n",
        "pd.set_option('display.max_columns', None)\n",
        "\n",
        "numeric_cols = [\n",
        "    'utc_offset', 'num_of_followers', 'num_of_people_following', 'num_of_status_updates', \n",
        "    'num_of_direct_messages', 'avg_daily_profile_visit_duration_in_seconds', \n",
        "    'avg_daily_profile_clicks'\n",
        "]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NqY7IFR5BAEo"
      },
      "source": [
        "class Imputer:\n",
        "    @staticmethod\n",
        "    def _fillna(df, col_name, value, na_symbol=None):\n",
        "        if na_symbol:\n",
        "            df[col_name].replace(na_symbol, np.NaN, inplace=True)\n",
        "        df[col_name].fillna(value, inplace=True)\n",
        "\n",
        "    def impute_data(self, df, train):\n",
        "        if train:\n",
        "            df.dropna(subset=['num_of_profile_likes'], axis=0, inplace=True)\n",
        "        self._fillna(df, col_name='profile_cover_image_status', value='Not set')\n",
        "        self._fillna(df, col_name='is_profile_view_size_customized?', value=False)\n",
        "        self._fillna(df, col_name='location_public_visibility', value='disabled', na_symbol='??')\n",
        "        self._fillna(df, col_name='profile_category', value='unknown', na_symbol=' ')\n",
        "        self._fillna(df, col_name='avg_daily_profile_visit_duration_in_seconds', value=df['avg_daily_profile_visit_duration_in_seconds'].median())\n",
        "        self._fillna(df, col_name='avg_daily_profile_clicks', value=df['avg_daily_profile_clicks'].median())\n",
        "\n",
        "        self._fillna(df, col_name='utc_offset', value=df.groupby(['utc_offset', 'location'])['utc_offset'].transform('max'))\n",
        "        self._fillna(df, col_name='utc_offset', value=0)\n",
        "        return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oF8IFHXhBCwW"
      },
      "source": [
        "class FeatureEngineer:\n",
        "    scaler = None\n",
        "\n",
        "    def __init__(self, scaler=None):\n",
        "        self.scaler = scaler or preprocessing.StandardScaler()\n",
        "\n",
        "    def process_data(self, df, train):\n",
        "        df['has_personal_url'] = df['personal_url'].notna().astype(int)\n",
        "        df['cover_image_set'] = df['profile_cover_image_status'].apply(lambda x: 1 if x == 'Set' else 0)\n",
        "        df['view_size_customized'] = df['is_profile_view_size_customized?'].astype(int)\n",
        "        df['is_location_visible'] = df['location_public_visibility'].apply(lambda x: 1 if x.lower() == 'enabled' else 0)\n",
        "\n",
        "        df = df.join(self._encode_category(df, 'profile_verification_status', prefix='status'))\n",
        "        df = df.join(self._encode_category(df, 'profile_category', prefix='category'))\n",
        "        df = df.join(self._encode_category(df, 'user_language', prefix='lang'))\n",
        "        df = df.join(self._encode_category(df, 'cluster', prefix='cluster'))\n",
        "        df['creation_year'] = df['profile_creation_timestamp'].apply(lambda x: x.year)\n",
        "        df['creation_year'] = df['creation_year'] - df['creation_year'].min()\n",
        "\n",
        "        df = self._normalize_columns(df, col_names=[\n",
        "            'utc_offset', 'num_of_followers', 'num_of_people_following',\n",
        "            'num_of_status_updates', 'num_of_direct_messages',\n",
        "            'avg_daily_profile_visit_duration_in_seconds', 'avg_daily_profile_clicks',\n",
        "        ], train=train)\n",
        "\n",
        "        self._drop_columns(df, col_names=[\n",
        "            'id', 'user_name', 'personal_url', 'profile_cover_image_status', \n",
        "            'is_profile_view_size_customized?', 'location_public_visibility', \n",
        "            'profile_verification_status', 'profile_text_color', 'profile_page_color', \n",
        "            'profile_theme_color',  'location', 'user_time_zone', 'profile_category', \n",
        "           'profile_image', 'profile_creation_timestamp', 'user_language', 'cluster',\n",
        "        ])\n",
        "        return df\n",
        "\n",
        "    @staticmethod\n",
        "    def _drop_columns(df, col_names):\n",
        "        df.drop(col_names, axis=1, inplace=True)\n",
        "\n",
        "    def _normalize_columns(self, df, col_names, train):\n",
        "        if train:\n",
        "            self.scaler.fit(df[col_names].values)\n",
        "        df.update(pd.DataFrame(self.scaler.transform(df[col_names].values), columns=col_names))\n",
        "        return df\n",
        "\n",
        "    @staticmethod\n",
        "    def _encode_category(df, col_name, target_encoding=False, prefix='profile'):\n",
        "        df[col_name] = df[col_name].apply(lambda x: x.replace(' ', '_').lower())\n",
        "        return pd.get_dummies(df[col_name], prefix=prefix)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eZ_p4puSBEmK"
      },
      "source": [
        "class DataPreprocessor:\n",
        "    imputer = None\n",
        "    feature_eng = None\n",
        "\n",
        "    def __init__(self, imputer=None, feature_eng=None):\n",
        "        self.imputer = imputer or Imputer()\n",
        "        self.feature_eng = feature_eng or FeatureEngineer()\n",
        "\n",
        "    def preprocess(self, df, train=False):\n",
        "        df = self._process_column_names(df)\n",
        "        df = self._handle_missing_values(df, train=train)\n",
        "        df = self._engineer_features(df, train=train)\n",
        "        return df\n",
        "\n",
        "    @staticmethod\n",
        "    def _process_column_names(df):\n",
        "        return df.rename(columns=lambda x: x.replace(' ', '_').lower())\n",
        "\n",
        "    def _handle_missing_values(self, df, train):\n",
        "        return self.imputer.impute_data(df, train=train)\n",
        "\n",
        "    def _engineer_features(self, df, train):\n",
        "        return self.feature_eng.process_data(df, train=train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eoRr5pFNWMN4"
      },
      "source": [
        "### Image feature extraction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Ixq7jX9BGLp"
      },
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.autograd import Variable\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import glob, os, time, copy\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "import os\n",
        "from PIL import Image\n",
        "import glob\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "import librosa\n",
        "from random import shuffle\n",
        "from skimage.io import imread\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "norm_dict = {\n",
        "'normalize_torch' : transforms.Normalize(\n",
        "    mean=[0.485, 0.456, 0.406],\n",
        "    std=[0.229, 0.224, 0.225]\n",
        "),\n",
        "'normalize_05' : transforms.Normalize(\n",
        "    mean=[0.5, 0.5, 0.5],\n",
        "    std=[0.5, 0.5, 0.5]\n",
        ")\n",
        "}\n",
        "\n",
        "class Read_dataset(Dataset):\n",
        "\n",
        "    def __init__(self, img_list, img_size=224, normalize_fun='normalize_torch'):\n",
        "        self.data = img_list\n",
        "        self.transform = transforms.Compose([\n",
        "\n",
        "            transforms.Resize((img_size,img_size), interpolation=2),\n",
        "            transforms.ToTensor(),\n",
        "            norm_dict[normalize_fun]\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        \n",
        "        img= imread(self.data[i])\n",
        "        \n",
        "        image = self.transform(Image.fromarray(img))\n",
        "\n",
        "        return image, self.data[i]\n",
        "\n",
        "class feature_model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.model = models.resnet50(pretrained=True)\n",
        "        self.feature = nn.Sequential(*list(self.model.children())[:-1])\n",
        "    # Set your own forward pass\n",
        "    def forward(self, x):\n",
        "        x = self.feature(x).view(-1, 2048)\n",
        "        return x # dictionary with the outputs from the 512 classifiers\n",
        "\n",
        "\n",
        "img_list = glob.glob('/content/train_profile_images/profile_images_train/*.png')\n",
        "test_img = glob.glob('/content/test_profile_images/profile_images_test/*.png')\n",
        "# print(len(img_list))\n",
        "train_dataset = Read_dataset(img_list)\n",
        "training_data_loader = DataLoader(dataset=train_dataset, num_workers=8,\n",
        "                                  batch_size=16,\n",
        "                                  shuffle=True)\n",
        "test_dataset = Read_dataset(test_img)\n",
        "test_data_loader = DataLoader(dataset=test_dataset, num_workers=8,\n",
        "                                  batch_size=16,\n",
        "                                  shuffle=True)\n",
        "\n",
        "d = {}\n",
        "val = np.zeros((7500, 2048))\n",
        "test_val = np.zeros((2500, 2048))\n",
        "\n",
        "count = 0\n",
        "x = feature_model().cuda()\n",
        "for i , j in training_data_loader:\n",
        "  i = i.cuda()\n",
        "  out = x(i)\n",
        "  out = out.data.cpu().numpy()\n",
        "  for num in range(len(j)):\n",
        "    key = j[num].split('/')[-1]\n",
        "    if key in d:\n",
        "      print('error')\n",
        "    d[key] = out[num]\n",
        "    val[count] = out[num]\n",
        "    count += 1\n",
        "\n",
        "count = 0\n",
        "# x = feature_model().cuda()\n",
        "for i , j in test_data_loader:\n",
        "  i = i.cuda()\n",
        "  out = x(i)\n",
        "  out = out.data.cpu().numpy()\n",
        "  for num in range(len(j)):\n",
        "    key = j[num].split('/')[-1]\n",
        "    if key in d:\n",
        "      print('error')\n",
        "    d[key] = out[num]\n",
        "    test_val[count] = out[num]\n",
        "    count += 1\n",
        "print('+ done ')\n",
        "\n",
        "n = 6\n",
        "kmeans = KMeans(n_clusters=n, random_state=0).fit(val)\n",
        "label = kmeans.labels_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQ5OsB5zXAVv"
      },
      "source": [
        "def rmsle(y_true, y_pred):\n",
        "    return np.sqrt(mean_squared_log_error(y_true, y_pred))\n",
        "\n",
        "\n",
        "def clip_columns(df):\n",
        "    cols = ['Num of Followers', 'Num of People Following', 'Num of Status Updates', 'Num of Direct Messages']\n",
        "    q = df[cols].quantile([0.25, 0.95])\n",
        "    for col in cols:\n",
        "        df[col] = df[col].clip(*q[col])\n",
        "    return df\n",
        "\n",
        "def img_cluster(kmeans_model, feature_dict, img_key):\n",
        "    value = feature_dict[img_key]\n",
        "    cluster = kmeans_model.predict(np.expand_dims(value,axis=0))\n",
        "    return cluster[0]\n",
        "\n",
        "def cluster_update(kmeans_model, feature_dict, df):\n",
        "    x = df['Profile Image'].values\n",
        "    lab =[]\n",
        "    # print(x)\n",
        "    for i in x :\n",
        "        # print(i)\n",
        "        lab.append(str(img_cluster(kmeans_model, feature_dict, i)))\n",
        "    return lab\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    df_train = pd.read_csv('./train.csv', parse_dates=['Profile Creation Timestamp'])\n",
        "    df_test = pd.read_csv('./test.csv', parse_dates=['Profile Creation Timestamp'])\n",
        "\n",
        "    df_train['cluster'] = cluster_update(kmeans, d, df_train)\n",
        "    df_test['cluster'] = cluster_update(kmeans, d, df_test)\n",
        "\n",
        "    test_id = df_test['Id'].copy()\n",
        "\n",
        "    # df_train = clip_columns(df_train)\n",
        "    # df_test = clip_columns(df_test)\n",
        "\n",
        "    preprocessor = DataPreprocessor()\n",
        "    df_train = preprocessor.preprocess(df_train, train=True)\n",
        "\n",
        "    pow_transform = PowerTransformer()\n",
        "    df_train[numeric_cols] = pow_transform.fit_transform(df_train[numeric_cols])\n",
        "\n",
        "    df_train['num_of_profile_likes'] = df_train['num_of_profile_likes'].apply(np.log1p)\n",
        "    X, y = df_train.loc[:, df_train.columns != 'num_of_profile_likes'], df_train['num_of_profile_likes']\n",
        "\n",
        "    df_test = preprocessor.preprocess(df_test, train=False)\n",
        "    df_test['lang_el'] = 0\n",
        "    df_test['lang_sk'] = 0\n",
        "    df_test['lang_uk'] = 0\n",
        "    df_test['lang_sr'] = 0\n",
        "    df_test['lang_zh-tw'] = 0\n",
        "    df_test['lang_da'] = 0\n",
        "    df_test.drop(columns=['lang_he', 'lang_no', 'lang_ro'], inplace=True)\n",
        "    df_test[numeric_cols] = pow_transform.transform(df_test[numeric_cols])\n",
        "    df_test = df_test[X.columns]\n",
        "    test_data = df_test\n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQ3tjqftXx05"
      },
      "source": [
        "### Model Selection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZNgh1YWaX0cZ"
      },
      "source": [
        "from sklearn.metrics import mean_squared_log_error\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.ensemble import ExtraTreesRegressor\n",
        "from sklearn.ensemble import AdaBoostRegressor\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "from lightgbm import LGBMRegressor\n",
        "from catboost import CatBoostRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import KFold, cross_val_score, StratifiedKFold\n",
        "\n",
        "def run_gradient_boosting(clf, fit_params, train, target, test):\n",
        "  N_SPLITS = 5\n",
        "  oofs = np.zeros(len(train))\n",
        "  preds = np.zeros((len(test)))\n",
        "\n",
        "  # target = train[TARGET_COL]\n",
        "  features = list(train.columns.values)\n",
        "  folds = StratifiedKFold(n_splits = N_SPLITS)\n",
        "  stratified_target = pd.qcut(target, 10, labels = False, duplicates='drop')\n",
        "\n",
        "  feature_importances = pd.DataFrame()\n",
        "\n",
        "  for fold_, (trn_idx, val_idx) in enumerate(folds.split(train, stratified_target)):\n",
        "    print(f'\\n------------- Fold {fold_ + 1} -------------')\n",
        "\n",
        "    ### Training Set\n",
        "    X_trn, y_trn = train.iloc[trn_idx], target.iloc[trn_idx]\n",
        "\n",
        "    ### Validation Set\n",
        "    X_val, y_val = train.iloc[val_idx], target.iloc[val_idx]\n",
        "    \n",
        "    _ = clf.fit(X_trn, y_trn, eval_set = [(X_val, y_val)], **fit_params)\n",
        "    fold_importance = pd.DataFrame({'fold': fold_ + 1, 'feature': features, 'importance': clf.feature_importances_})\n",
        "    feature_importances = pd.concat([feature_importances, fold_importance], axis=0)\n",
        "\n",
        "    ### Instead of directly predicting the classes we will obtain the probability of positive class.\n",
        "    preds_val = clf.predict(X_val)\n",
        "    # pred_val[pred_val<0] = 0\n",
        "    preds_test = clf.predict(test)\n",
        "\n",
        "    fold_score = av_metric(y_val, preds_val)\n",
        "    print(f'\\nAV metric score for validation set is {fold_score}')\n",
        "\n",
        "    oofs[val_idx] = preds_val\n",
        "    preds += preds_test / N_SPLITS\n",
        "\n",
        "\n",
        "  oofs_score = av_metric(target, oofs)\n",
        "  print(f'\\n\\nAV metric for oofs is {oofs_score}')\n",
        "\n",
        "  feature_importances = feature_importances.reset_index(drop = True)\n",
        "  fi = feature_importances.groupby('feature')['importance'].mean().sort_values(ascending = False)[:20][::-1]\n",
        "  fi.plot(kind = 'barh', figsize=(12, 6))\n",
        "\n",
        "  return oofs, preds , fi\n",
        "\n",
        "def av_metric(y_true, y_pred):\n",
        "  return np.sqrt(mean_squared_log_error(np.exp(y_true), np.exp(y_pred)))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LyxENJyeazyP"
      },
      "source": [
        "### _XGBoostRegressor_"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NkEy7ziZazHw"
      },
      "source": [
        "model = XGBRegressor(n_estimators = 1000,\n",
        "                    max_depth = 6,\n",
        "                    learning_rate = 0.05,\n",
        "                    colsample_bytree = 0.5,\n",
        "                    random_state=1452,\n",
        "                    )\n",
        "\n",
        "fit_params = {'verbose': 200, 'early_stopping_rounds': 200}\n",
        "\n",
        "xgb_oofs, pred_xgb, fi = run_gradient_boosting(model, fit_params, X, y, test_data)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o78e38zRbHDJ"
      },
      "source": [
        "### _LGBMRegressor_"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dW89heMYa7Im"
      },
      "source": [
        "model = LGBMRegressor(n_estimators = 5000,\n",
        "                        learning_rate = 0.01,\n",
        "                        colsample_bytree = 0.76,\n",
        "                        metric = 'None',\n",
        "                        )\n",
        "fit_params = {'verbose': 300, 'early_stopping_rounds': 200, 'eval_metric': 'rmse'}\n",
        "lgb_oofs, pred_lgb, fi = run_gradient_boosting(model, fit_params, X, y, test_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I9d1ocOibCTF"
      },
      "source": [
        "### _CatBoostRegressor_"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LXRcY3N2bB1H"
      },
      "source": [
        "model = CatBoostRegressor(n_estimators = 3000,\n",
        "                       learning_rate = 0.01,\n",
        "                       rsm = 0.4, ## Analogous to colsample_bytree\n",
        "                       random_state=2054,\n",
        "                       )\n",
        "\n",
        "fit_params = {'verbose': 200, 'early_stopping_rounds': 200}\n",
        "\n",
        "cb_oofs,pred_cb, fi = run_gradient_boosting(model, fit_params, X, y, test_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oGWuYp6ebYTU"
      },
      "source": [
        "### _Submission - 3_"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GHOUKSJpbKla"
      },
      "source": [
        "v = np.round((np.exp(pred_xgb)-1 + np.exp(pred_lgb)-1 + np.exp(pred_cb)-1)/3).astype('int')\n",
        "x = pd.read_csv('./sample_submission.csv')\n",
        "x['Predicted'] = v\n",
        "x.to_csv('ensemble_try.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZqeXavebfFKF"
      },
      "source": [
        "# **Annex A - Best private result on Kaggle - 1.60690 (unsubmitted)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sOaAsDn9hUWZ"
      },
      "source": [
        "Readme:\r\n",
        "\r\n",
        "\r\n",
        "1.   Run all the cells from \"Environment & Global Variables Config\", \"Preprocessing Functions\" & \"Model Function\" to define functions.\r\n",
        "2.  To start processing data, run the \"Main\" cells at the bottom.\r\n",
        "3.  The preprocessing ouput variables are called prep_df_train & prep_df_test. Those variables will be inputed in the model.\r\n",
        "4.  The submission will be generated running the last cell of \"Main code\". The file will appear in the Colab left pannel and can be downloaded.\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_9lRC9-gwaZ"
      },
      "source": [
        "## Environment & Global Variables Config"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ln8Yr-a4gwaZ"
      },
      "source": [
        "### Package Installation & Import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "clDa2L2ngwaZ"
      },
      "source": [
        "!pip install catboost\n",
        "!pip install lightgbm\n",
        "!pip install langcodes\n",
        "import gdown\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import langcodes\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import Normalizer\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.preprocessing import PowerTransformer\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import SpectralClustering\n",
        "from sklearn.model_selection import (cross_val_score,StratifiedKFold)\n",
        "from sklearn.metrics import mean_squared_log_error\n",
        "from sklearn.ensemble import ExtraTreesRegressor\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from sklearn.feature_selection import SelectKBest, mutual_info_regression  \n",
        "\n",
        "from sklearn.metrics import mean_squared_log_error\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.ensemble import ExtraTreesRegressor\n",
        "from sklearn.ensemble import AdaBoostRegressor\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import (\n",
        "    RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor, \n",
        "    VotingRegressor, StackingRegressor, BaggingRegressor,\n",
        ")\n",
        "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
        "from sklearn.metrics import make_scorer\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.svm import SVR, LinearSVR\n",
        "from xgboost import XGBRegressor\n",
        "from  matplotlib import pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn import linear_model\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import RobustScaler, MinMaxScaler\n",
        "from scipy.special import boxcox1p, inv_boxcox1p\n",
        "from scipy.stats import boxcox_normmax\n",
        "from scipy.stats import skew, norm\n",
        "from scipy.stats import yeojohnson\n",
        "from sklearn.preprocessing import PowerTransformer\n",
        "from lightgbm import LGBMRegressor\n",
        "from catboost import CatBoostRegressor\n",
        "from sklearn.linear_model import RidgeCV\n",
        "\n",
        "\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import random\n",
        "#from google.colab import drive\n",
        "\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "# Set random seeds (for reproducibility requirement)\n",
        "os.environ['PYTHONHASHSEED']=str(1)\n",
        "tf.random.set_seed(1)\n",
        "np.random.seed(1)\n",
        "random.seed(1)\n",
        "#TO INCLUDE OTHER LIBRARIES AS WELL\n",
        "\n",
        "#drive.mount('/content/drive/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3TEJj4Udgwaa"
      },
      "source": [
        "### Datasets download"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qxQM2xXxgwaa"
      },
      "source": [
        "#gdown.download(\"https://drive.google.com/uc?id=1hE7ZuRTD8uBKLmls0NUrMCJEr4iXJDMn\",\"train.csv\",quiet=True);  #importing files from local Google Drive.\n",
        "#gdown.download(\"https://drive.google.com/uc?id=105G0nu5i7It_ZUZgnF63ARYqABHNU4cB\",\"test.csv\",quiet=True);\n",
        "gdown.download(\"https://drive.google.com/uc?id=1jQf-0Xw0wNymVGeAe_NmmFYOf5lJm5Ty\",\"dataset.zip\",quiet=True);\n",
        "!unzip -qq dataset.zip\n",
        "\n",
        "#!cp -R /content/drive/MyDrive/Python/IFT6758/ift6758-a20.zip .   #Autre mthode"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_3bEQHemgwaa"
      },
      "source": [
        "### Global variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9-UX8XUsgwaa"
      },
      "source": [
        "def get_images(img_serie, path):\n",
        "  img_df = [cv2.imread(path+img_name) for img_name in img_serie]\n",
        "  return img_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jz5fLThkgwaa"
      },
      "source": [
        "df_train = pd.read_csv(\"train.csv\")\n",
        "df_test = pd.read_csv(\"test.csv\") \n",
        "\n",
        "img_train = get_images(df_train['Profile Image'],\"./train_profile_images/profile_images_train/\")\n",
        "img_test = get_images(df_test['Profile Image'],\"./test_profile_images/profile_images_test/\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D0sjX4Vbgwaa"
      },
      "source": [
        "## Data Exploration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17Xwx-qCgwaa"
      },
      "source": [
        "### Training Dataset Exploration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yoDkCsHBgwaa"
      },
      "source": [
        "df_train.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rRYMloqVgwaa"
      },
      "source": [
        "df_train.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZTmjvCFZgwab"
      },
      "source": [
        "sns.boxplot(x='Profile Category',y='Num of Profile Likes',hue='Is Profile View Size Customized?',data=df_train);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3yMkL3Dugwab"
      },
      "source": [
        "sns.boxplot(y='Num of Profile Likes',data=df_train);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ylnhca6Kgwab"
      },
      "source": [
        "#viewing Likes distribution along with URL (identified as important feature)\n",
        "g = sns.kdeplot(df_train['Num of Profile Likes'][df_train['Personal URL'].str.len() > 12], label=\"URL Defined\", shade=True, color=\"red\")\n",
        "g = sns.kdeplot(df_train['Num of Profile Likes'][~(df_train['Personal URL'].str.len() > 12)], label=\"URL Not Defined\", shade=True, color=\"green\")\n",
        "plt.xlabel(\"Likes\")\n",
        "plt.ylabel(\"Density\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "#Distribution is very large -> log scale for Likes?"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-B7hSAogwab"
      },
      "source": [
        "g = sns.kdeplot(np.log(1+df_train['Num of Profile Likes'][df_train['Personal URL'].str.len() > 12]), label=\"URL Defined\", shade=True, color=\"red\")\n",
        "g = sns.kdeplot(np.log(1+df_train['Num of Profile Likes'][~(df_train['Personal URL'].str.len() > 12)]), label=\"URL Not Defined\", shade=True, color=\"green\")\n",
        "plt.xlabel(\"Likes\")\n",
        "plt.ylabel(\"Density\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "#Much better in log scale. We can as well confirm the URL presence is an important feature."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YqPJmghxgwab"
      },
      "source": [
        "list(img_train[0][0][0]) == [1,1,1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3rHJZm1Lgwab"
      },
      "source": [
        "df_train_img.head(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CaWQfPjIgwab"
      },
      "source": [
        "### Test Dataset Exploration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4TclFPKDgwab"
      },
      "source": [
        "#Check missing values & other coner cases that would have been missed with Training Dataset.\n",
        "\n",
        "df_test.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2RPuo4Zgwac"
      },
      "source": [
        "### Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C5VeezHjgwac"
      },
      "source": [
        "#User Name feature\n",
        "temp_fea = df_train['User Name'].apply(lambda x:len(x))\n",
        "temp_fea.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n7BTrS_dgwac"
      },
      "source": [
        "#Personnal URL\n",
        "temp_fea = df_train['Personal URL'].str.len() > 12      #to consider URL shorter than 12 chars as irrelevant.\n",
        "temp_fea.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xj5p0Ncygwac"
      },
      "source": [
        "#Profile Cover Image Status -> missing values = Unknown category\n",
        "temp_fea = df_train['Profile Cover Image Status'].copy()\n",
        "temp_fea[(temp_fea != 'Set') & (temp_fea != 'Not set')] = 'Unknown'\n",
        "\n",
        "print(temp_fea.head(5))\n",
        "temp_fea[temp_fea == 'Unknown']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ViV5YKXngwac"
      },
      "source": [
        "#Profile Verification Status -> to be One-Hot Encoded\n",
        "temp_fea = df_train['Profile Verification Status'].copy()\n",
        "temp_fea.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gcMMOGASgwac"
      },
      "source": [
        "#temp_fea = df_train[['Profile Text Color','Profile Page Color','Profile Theme Color']].apply(lambda x:['000000' if pd.isnull(x) & (len(str(x)) > 6) else x,axis=1,result_type='broadcast')\n",
        "temp_fea = pd.DataFrame()\n",
        "temp = df_train[['Profile Text Color','Profile Page Color','Profile Theme Color']].applymap(lambda x:'000000' if (pd.isnull(x) | (len(str(x)) != 6)) else x)\n",
        "reds,greens,blues = ['Text Red','Page Red','Theme Red'],['Text Green','Page Green','Theme Green'],['Text Blue','Page Blue','Theme Blue']\n",
        "temp_fea[reds],temp_fea[greens],temp_fea[blues] = temp.applymap(lambda x:int(x[0:2],16)),temp.applymap(lambda x:int(x[2:4],16)),temp.applymap(lambda x:int(x[4:6],16))\n",
        "temp_fea"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jFhrT0Q-gwac"
      },
      "source": [
        "#Is Profile View Size Customized? already in boolean, which is good\n",
        "temp_fea = df_train['Is Profile View Size Customized?']\n",
        "temp_fea.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mQNx1vYfgwac"
      },
      "source": [
        "#UTC Offset, Location & User Time Zone -> drop location, map UTC Offset and Time zone to complete UTC Offset missing data. Then convert UTC Offset to global regions.\n",
        "tz_mapping = df_train[['UTC Offset','User Time Zone']].pivot_table(index=['User Time Zone','UTC Offset'],values='UTC Offset',aggfunc='count').reset_index(drop=False)\n",
        "utc_labels = ['America','Europe & Africa','Middle East','East Asia']\n",
        "temp_fea = df_train['User Time Zone'].apply(lambda x:utc_labels[np.digitize(tz_mapping[tz_mapping['User Time Zone']==x]['UTC Offset'].values[0],[-5400,9000,23400])] if not pd.isnull(x) else 'Unknown')\n",
        "temp_fea\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T3rVqQJOgwac"
      },
      "source": [
        "#Location Public Visibility & User Language -> lowercase & empty values management\n",
        "temp_fea = df_train[['Location Public Visibility','User Language']].applymap(lambda x:x.lower() if not pd.isnull(x) else 'none')\n",
        "temp_fea\n",
        "lb = LabelBinarizer()\n",
        "temp = lb.fit_transform(temp_fea['Location Public Visibility'])\n",
        "lb.classes_\n",
        "temp_fea = pd.concat([temp_fea,pd.DataFrame(temp,columns=lb.classes_)],axis=1).drop(['Location Public Visibility'], axis=1)\n",
        "temp_fea"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_xauHsfNgwac"
      },
      "source": [
        "#Profile Creation Timestamp\n",
        "temp_fea = df_train['Profile Creation Timestamp'].apply(lambda x:int(x[-4:]))\n",
        "temp_fea.unique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7UeXATZdgwah"
      },
      "source": [
        "#Num of Followers, Num of People Following, Num of Status Updates, Num of Direct Messages, Avg Daily Profile Visit Duration in seconds & Avg Daily Profile Clicks\n",
        "temp_fea = df_train['Num of Followers']\n",
        "temp = temp_fea.fillna(value=temp_fea.median())\n",
        "\n",
        "temp = StandardScaler().fit_transform(np.array(temp.copy()).reshape(-1,1))\n",
        "temp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V5_FLwG-gwah"
      },
      "source": [
        "#Profile Category\n",
        "df_train['Profile Category'].unique()\n",
        "temp_fea = df_train['Profile Category'].apply(lambda x:x.lower() if (not pd.isnull(x) | (x==' ')) else 'unknown')\n",
        "temp_fea.unique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FnOxGWlHgwah"
      },
      "source": [
        "#Profile Likes -> change to Log scale\n",
        "temp_fea = np.log(1+df_train['Num of Profile Likes'])\n",
        "plt.hist(temp_fea,bins=50);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvJiXYKEgwah"
      },
      "source": [
        "sns.boxplot(temp_fea)\n",
        "#Even if outliers, we might consider not dropping zeros, as they might contain useful information for model.\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O4GsbOd6gwai"
      },
      "source": [
        "Q1 = temp_fea.quantile(0.25)\n",
        "Q3  = temp_fea.quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "#print(Q3 +1.5*IQR)\n",
        "#print(Q1 - 1.5*IQR)\n",
        "print(temp_fea[(temp_fea > Q3 +1.5*IQR)])\n",
        "#We could drop upper outlier only for Log Likes."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nKEm_Ugfgwai"
      },
      "source": [
        "#Zero number of Likes: To drop or not to drop.\n",
        "print(len(df_train['Personal URL'][(temp_fea == 0) & (df_train['Personal URL'].str.len() > 12)]))\n",
        "print(len(df_train['Personal URL'][(temp_fea == 0) & ~(df_train['Personal URL'].str.len() > 12)]))\n",
        "#80% have an URL\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tl-Dx39_gwai"
      },
      "source": [
        "#Same process with all numerical features transformation.\n",
        "#['Num of Followers','Num of People Following','Num of Status Updates','Num of Direct Messages','Avg Daily Profile Visit Duration in seconds','Avg Daily Profile Clicks','Num of Profile Likes']\n",
        "fig,ax = plt.subplots(2,3,figsize=(20,7))\n",
        "sns.kdeplot(df_train['Num of Followers'], label=\"URL Defined\", shade=True, color=\"red\",ax=ax[0,0])\n",
        "sns.kdeplot(df_train['Num of People Following'], label=\"URL Defined\", shade=True, color=\"red\",ax=ax[0,1])\n",
        "sns.kdeplot(df_train['Num of Status Updates'], label=\"URL Defined\", shade=True, color=\"red\",ax=ax[0,2])\n",
        "sns.kdeplot(df_train['Num of Direct Messages'], label=\"URL Defined\", shade=True, color=\"red\",ax=ax[1,0])\n",
        "sns.kdeplot(df_train['Avg Daily Profile Visit Duration in seconds'], label=\"URL Defined\", shade=True, color=\"red\",ax=ax[1,1])\n",
        "sns.kdeplot(df_train['Avg Daily Profile Clicks'], label=\"URL Defined\", shade=True, color=\"red\",ax=ax[1,2])\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "codx2CeVgwai"
      },
      "source": [
        "In conclusion, Log transformation must by applied to all numerical feature except 'Avg Daily Profile Visit Duration in seconds'."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "viTg96zhgwai"
      },
      "source": [
        "fig,ax = plt.subplots(2,3,figsize=(20,7))\n",
        "sns.kdeplot(np.log(1+df_train['Num of Followers']), label=\"URL Defined\", shade=True, color=\"red\",ax=ax[0,0])\n",
        "sns.kdeplot(np.log(1+df_train['Num of People Following']), label=\"URL Defined\", shade=True, color=\"red\",ax=ax[0,1])\n",
        "sns.kdeplot(np.log(1+df_train['Num of Status Updates']), label=\"URL Defined\", shade=True, color=\"red\",ax=ax[0,2])\n",
        "sns.kdeplot(np.log(1+df_train['Num of Direct Messages']), label=\"URL Defined\", shade=True, color=\"red\",ax=ax[1,0])\n",
        "sns.kdeplot(df_train['Avg Daily Profile Visit Duration in seconds'], label=\"URL Defined\", shade=True, color=\"red\",ax=ax[1,1])\n",
        "sns.kdeplot(np.log(1+df_train['Avg Daily Profile Clicks']), label=\"URL Defined\", shade=True, color=\"red\",ax=ax[1,2])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jElwj1MYgwai"
      },
      "source": [
        "#feature extraction on image -> img_train[image#][line#][pixel#][color#(RBG)]\n",
        "#Average color\n",
        "temp_fea = []\n",
        "for image in img_train:\n",
        "  red = 0\n",
        "  green = 0\n",
        "  blue = 0\n",
        "  for line in image:\n",
        "    for pixel in line:\n",
        "      red += pixel[0]\n",
        "      green += pixel[1]\n",
        "      blue += pixel[2]\n",
        "  temp_fea.append([red/(32*32),green/(32*32),blue/(32*32)])\n",
        "\n",
        "#red = red / (len(img_train)*len(img_train[0])*len(img_train[0][0]))\n",
        "#green = green / (len(img_train)*len(img_train[0])*len(img_train[0][0]))\n",
        "#blue = blue / (len(img_train)*len(img_train[0])*len(img_train[0][0]))\n",
        "#print('Avg red color per pixel: ',red,'\\nAvg red color per pixel: ',green,'\\nAvg red color per pixel:',blue)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17hviREdgwai"
      },
      "source": [
        "temp_fea = pd.DataFrame(temp_fea, columns=['Red','Green','Blue'])\n",
        "temp_fea.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1L_y8Nigwai"
      },
      "source": [
        "fig,ax = plt.subplots(1,3,figsize=(20,7))\n",
        "sns.kdeplot(temp_fea[\"Red\"], label=\"Red\", shade=True, color=\"red\",ax=ax[0])\n",
        "sns.kdeplot(temp_fea[\"Green\"], label=\"Green\", shade=True, color=\"green\",ax=ax[1])\n",
        "sns.kdeplot(temp_fea[\"Blue\"], label=\"Blue\", shade=True, color=\"blue\",ax=ax[2])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LawNjUBhgwaj"
      },
      "source": [
        "num_fea = ['Red','Green','Blue','Num of Profile Likes']\n",
        "sns.pairplot(pd.concat([temp_fea,np.log(1+df_train['Num of Profile Likes'])],axis=1),x_vars=num_fea,y_vars=num_fea)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkVYF734gwaj"
      },
      "source": [
        "Not much correlation between colors and number of Likes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7jAAawK_gwaj"
      },
      "source": [
        "#Number of black pixels exploration -> shape indicator\n",
        "temp_fea = []\n",
        "\n",
        "for image in img_train:\n",
        "  black_one = 0\n",
        "  black_zero = 0\n",
        "  for line in image:\n",
        "    for pixel in line:\n",
        "      if list(pixel) == [1,1,1]:\n",
        "        black_one += 1\n",
        "      elif list(pixel) == [0,0,0]:\n",
        "        black_zero += 1\n",
        "  temp_fea.append([black_zero,black_one])\n",
        "#black = sum(temp_fea) / (len(img_train))\n",
        "#print('Avg black pixels per image: ',black)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KjrRyURFgwaj"
      },
      "source": [
        "temp_fea = pd.DataFrame(temp_fea,columns=['black_zero','black_one'])\n",
        "bool_bg = temp_fea['black_zero'] > temp_fea['black_one']\n",
        "print(bool_bg)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nUUNz-W9gwaj"
      },
      "source": [
        "g = sns.kdeplot(np.log(1+df_train['Num of Profile Likes'][bool_bg]), label=\"0 background\", shade=True, color=\"red\")\n",
        "g = sns.kdeplot(np.log(1+df_train['Num of Profile Likes'][~bool_bg]), label=\"1 background\", shade=True, color=\"green\")\n",
        "plt.xlabel(\"Likes\")\n",
        "plt.ylabel(\"Density\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5urXjD_tgwaj"
      },
      "source": [
        "Not much correlation with background color = [1,1,1] or [0,0,0]."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Icbv4DPlgwaj"
      },
      "source": [
        "num_fea = ['black_zero','black_one','Num of Profile Likes']\n",
        "sns.pairplot(pd.concat([temp_fea,np.log(1+df_train['Num of Profile Likes'])],axis=1),x_vars=num_fea,y_vars=num_fea)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gMwoGNu1gwaj"
      },
      "source": [
        "g = sns.relplot(x=temp_fea['black_one'], y=np.log(1+df_train['Num of Profile Likes']), kind=\"line\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qK3wFMtrgwak"
      },
      "source": [
        "len(temp_fea.loc[temp_fea[1] < 200])+len(temp_fea.loc[temp_fea[0] < 200])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KCZVNiflgwak"
      },
      "source": [
        "#Image shape categorization\n",
        "\n",
        "#1- convert to black and white\n",
        "#2- correlate with number of black pixel +-5% OR use clustering with number of different shape (face types) -> too many different shapes\n",
        "#3- Try to get male of female information\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZppGNbgUgwak"
      },
      "source": [
        "#Cross validation split\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from sklearn.decomposition import PCA\r\n",
        "from sklearn.cluster import SpectralClustering\r\n",
        "from sklearn.model_selection import cross_val_score\r\n",
        "from sklearn.metrics import mean_squared_log_error\r\n",
        "from sklearn.ensemble import ExtraTreesRegressor\r\n",
        "from sklearn.cluster import KMeans\r\n",
        "from sklearn.cluster import AgglomerativeClustering\r\n",
        "\r\n",
        "# Create dataset to use : PCA(n=2) reduced digits\r\n",
        "X = df_train_img.iloc[:,0:3072]\r\n",
        "\r\n",
        "pca = PCA(n_components=5)\r\n",
        "X_r = pca.fit(X).transform(X)\r\n",
        "\r\n",
        "# Split the dataset in two equal parts\r\n",
        "X_train, X_test, y_train, y_test = train_test_split(\r\n",
        "    X_r, df_train_img['Num of Profile Likes'], test_size=0.5, random_state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SDwGRXCSgwak"
      },
      "source": [
        "#Grid Search\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.svm import SVR\n",
        "\n",
        "from sklearn.cluster import SpectralClustering\n",
        "\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Create dataset to use : PCA(n=2) reduced digits\n",
        "X = df_train_img.iloc[:,0:3072]\n",
        "\n",
        "pca = PCA(n_components=5)\n",
        "X_r = pca.fit(X).transform(X)\n",
        "\n",
        "# Split the dataset in two equal parts\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_r, df_train_img['Num of Profile Likes'], test_size=0.5, random_state=0)\n",
        "\n",
        "# Set the parameters by cross-validation\n",
        "''' tuned_parameters = [\n",
        "  {'kernel': ['rbf'],\n",
        "  'gamma': [1e-3, 1e-4],\n",
        "  'C': [1, 2, 10, 100, 1000]},\n",
        "  \n",
        "  {'kernel': ['linear'],\n",
        "   'C': [1, 2, 10, 100, 1000]},\n",
        "] '''\n",
        "tuned_parameters = [\n",
        "  {'n_clusters': np.arange(100,501,100)}\n",
        "]\n",
        "\n",
        "metric = 'neg_mean_squared_log_error'\n",
        "\n",
        "cv_strategy = KFold(n_splits=4, shuffle=True)\n",
        "\n",
        "grid_search = GridSearchCV(\n",
        "    KMeans(), tuned_parameters, scoring=metric, cv=cv_strategy\n",
        ")\n",
        "grid_search.fit(X_train, y_train);\n",
        "print('Finished!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Wz9dZi6gwak"
      },
      "source": [
        "print(\"Best parameters set found on development set:\")\n",
        "print()\n",
        "print(grid_search.best_params_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oN7Gr7uYgwak"
      },
      "source": [
        "print(\"Grid scores on development set:\")\r\n",
        "print()\r\n",
        "means = grid_search.cv_results_['mean_test_score']\r\n",
        "stds = grid_search.cv_results_['std_test_score']\r\n",
        "for mean, std, params in zip(means, stds, grid_search.cv_results_['params']):\r\n",
        "    print(\"%0.3f (+/-%0.03f) for %r\"\r\n",
        "          % (mean, std * 2, params))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rxPbggx-gwak"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1crWbnWugwak"
      },
      "source": [
        "# tester : SpectralClustering\r\n",
        "#AgglomerativeClustering ->simple,ward"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CTZk2b5Vgwak"
      },
      "source": [
        "from sklearn import datasets\r\n",
        "from sklearn.decomposition import PCA\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "# Create dataset to use : PCA(n=2) reduced digits\r\n",
        "X = df_train_img.iloc[:,0:3072]\r\n",
        "\r\n",
        "pca = PCA(n_components=5)\r\n",
        "X_r = pca.fit(X).transform(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6RqV9iotgwal"
      },
      "source": [
        "from sklearn.cluster import KMeans\r\n",
        "\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "\r\n",
        "X_train, X_valid = train_test_split(X_r, test_size=0.33, random_state=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9A5OdkQegwal"
      },
      "source": [
        "valid_scores = []\r\n",
        "\r\n",
        "for k in np.arange(1,100):\r\n",
        "\r\n",
        "  kmeans = KMeans(n_clusters=k)\r\n",
        "  kmeans.fit(X_train)\r\n",
        "  valid_scores.append(kmeans.score(X_valid))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j4PVjz2ogwal"
      },
      "source": [
        "# Plotting the validation scores \r\n",
        "import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "plt.figure(figsize=(10,8))\r\n",
        "\r\n",
        "plt.plot(valid_scores, c='g')\r\n",
        "plt.xticks(np.arange(1,100,5))\r\n",
        "plt.xlabel('k')\r\n",
        "plt.ylabel('score')\r\n",
        "plt.title('Validation score for different values of k with K-Means for PCA(n=5) images DF')\r\n",
        "plt.plot();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7KiIAnHogwal"
      },
      "source": [
        "valid_scores = []\r\n",
        "\r\n",
        "for k in np.arange(1,100):\r\n",
        "\r\n",
        "  kmeans = KMeans(n_clusters=k)\r\n",
        "  kmeans.fit(X_train)\r\n",
        "  valid_scores.append(kmeans.score(X_valid))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rhgRg621gwal"
      },
      "source": [
        "prep_img_df.drop(prep_img_df.iloc[:,0:3072],axis=1).head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fyEtBfvRgwal"
      },
      "source": [
        "#ML for image feature extraction\r\n",
        "import tensorflow as tf\r\n",
        "from sklearn.metrics import silhouette_score\r\n",
        "import cv2\r\n",
        "\r\n",
        "images  = np.array(np.float32(img_train).reshape(len(img_train), -1)/255)\r\n",
        "\r\n",
        "model = tf.keras.applications.MobileNetV2(include_top=False,weights='imagenet', input_shape=(32, 32, 3))\r\n",
        "predictions = model.predict(images.reshape(-1, 32, 32, 3))\r\n",
        "pred_images = predictions.reshape(images.shape[0], -1)\r\n",
        "\r\n",
        "# Split the dataset in two equal parts\r\n",
        "X_train, X_test, y_train, y_test = train_test_split(\r\n",
        "    pred_images, df_train_img['Num of Profile Likes'], test_size=0.5, random_state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4lrOuzYPgwal"
      },
      "source": [
        "len(predictions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yFoNeuZUgwal"
      },
      "source": [
        "For KMeans score : increase becomes linear at K = 20"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QqgTt1thgwam"
      },
      "source": [
        "''' X_train, X_test, y_train, y_test = train_test_split(prep_df[cols_x], prep_df[cols_y], \r\n",
        "                                                    test_size=0.3, \r\n",
        "                                                    shuffle=True, \r\n",
        "                                                    random_state=76 #  To guarantee that the split will always be the same\r\n",
        "                                                    )   '''\r\n",
        "\r\n",
        "result = pd.DataFrame(columns = (\"Classifiers\",\"Training\",\"Testing\"))\r\n",
        "train_scores, test_scores = [],[]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FLbmKk58gwam"
      },
      "source": [
        "#Spectral Clustering\r\n",
        "\r\n",
        "clf_ext = ExtraTreesRegressor(\r\n",
        "    max_features='auto',\r\n",
        "    bootstrap=True,\r\n",
        "    oob_score=True,\r\n",
        "    #n_estimators=1000,\r\n",
        "    #max_depth=None,\r\n",
        "    max_depth = 10,\r\n",
        "    min_samples_leaf = 10,\r\n",
        "    n_estimators = 2000,\r\n",
        "    #min_samples_split=10\r\n",
        "    min_samples_split = 20\r\n",
        "    #class_weight=\"balanced\",\r\n",
        "    #min_weight_fraction_leaf=0.02\r\n",
        "    )\r\n",
        "\r\n",
        "train_scores, test_scores = [],[]\r\n",
        "\r\n",
        "for k in np.arange(1,30,5):\r\n",
        "  kspec = SpectralClustering(n_clusters=k,random_state=1,n_neighbors=5,affinity='nearest_neighbors')\r\n",
        "  clusters = pd.DataFrame(kspec.fit_predict(X_train,y_train))\r\n",
        "  #temp_df = pd.concat([y_train,clusters],axis=1)\r\n",
        "  \r\n",
        "  #clf_ext = clf_ext.fit(np.array(temp_df[0]).reshape(-1, 1),temp_df['Num of Profile Likes'])\r\n",
        "  clf_ext = clf_ext.fit(clusters,y_train)\r\n",
        "\r\n",
        "  train_scores.append(np.sqrt(mean_squared_log_error(y_train, clf_ext.predict(clusters).reshape(-1, 1))))\r\n",
        "  test_scores.append(np.sqrt(mean_squared_log_error(y_test, clf_ext.predict(kspec.fit_predict(X_test).reshape(-1, 1)))))\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BtlZkPc-gwam"
      },
      "source": [
        "print('Train RSMLE: ',train_scores)\r\n",
        "print('Test RSMLE: ',test_scores)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZZi1FKKgwam"
      },
      "source": [
        "#K-Means Clustering\r\n",
        "\r\n",
        "clf_ext = ExtraTreesRegressor(\r\n",
        "    max_features='auto',\r\n",
        "    bootstrap=True,\r\n",
        "    oob_score=True,\r\n",
        "    #n_estimators=1000,\r\n",
        "    #max_depth=None,\r\n",
        "    max_depth = 10,\r\n",
        "    min_samples_leaf = 10,\r\n",
        "    n_estimators = 2000,\r\n",
        "    #min_samples_split=10\r\n",
        "    min_samples_split = 20\r\n",
        "    #class_weight=\"balanced\",\r\n",
        "    #min_weight_fraction_leaf=0.02\r\n",
        "    )\r\n",
        "\r\n",
        "train_scores, test_scores = [],[]\r\n",
        "\r\n",
        "for k in np.arange(1,30,5):\r\n",
        "  kmeans = KMeans(n_clusters=k,random_state=1)\r\n",
        "  clusters = pd.DataFrame(kmeans.fit_predict(X_train,y_train))\r\n",
        "  #temp_df = pd.concat([y_train,clusters],axis=1)\r\n",
        "  \r\n",
        "  #clf_ext = clf_ext.fit(np.array(temp_df[0]).reshape(-1, 1),temp_df['Num of Profile Likes'])\r\n",
        "  clf_ext = clf_ext.fit(clusters,y_train)\r\n",
        "\r\n",
        "  train_scores.append(np.sqrt(mean_squared_log_error(y_train, clf_ext.predict(clusters).reshape(-1, 1))))\r\n",
        "  test_scores.append(np.sqrt(mean_squared_log_error(y_test, clf_ext.predict(kmeans.fit_predict(X_test).reshape(-1, 1)))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xNPWbkc9gwam"
      },
      "source": [
        "print('Train RSMLE: ',train_scores)\r\n",
        "print('Test RSMLE: ',test_scores)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W77tr77ygwam"
      },
      "source": [
        "#Final model testing\r\n",
        "\r\n",
        "from sklearn.cluster import KMeans\r\n",
        "from sklearn.cluster import SpectralClustering\r\n",
        "from sklearn.cluster import AgglomerativeClustering\r\n",
        "\r\n",
        "kmeans = KMeans(n_clusters=20)\r\n",
        "kspec = SpectralClustering(n_clusters=20)\r\n",
        "kagg = AgglomerativeClustering(n_clusters=20)\r\n",
        "\r\n",
        "kmeans.fit(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8oJrW_i6gwam"
      },
      "source": [
        "#correlation with nbr of likes\r\n",
        "df_train_img = pd.concat([df_train['Num of Profile Likes'],pd.DataFrame(np.array(img_train).reshape([7500,-1]))],axis=1)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "za1Z12Zigwam"
      },
      "source": [
        "#image pixel bits inversion\r\n",
        "inv_df_img = pd.DataFrame(np.array(img_train).reshape([7500,-1])).applymap(lambda x:sum(1<<(8-1-i) for i in range(8) if x>>i&1))\r\n",
        "inv_df_img = pd.concat([df_train['Num of Profile Likes'],inv_df_img],axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bLA8EkXXgwam"
      },
      "source": [
        "corr_train = df_train_img.corr().abs().sort_values(by=[\"Num of Profile Likes\"],ascending=False)\r\n",
        "corr_train.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "po_8YsCXgwan"
      },
      "source": [
        "top = 100\r\n",
        "bool_ar = np.array([255 if (i in corr_train.head(top).index) else 0 for i in range(0,len(df_train_img.columns)-1)])\r\n",
        "bool_ar = bool_ar.reshape(32,32,3)\r\n",
        "print(corr_train.head(top)[\"Num of Profile Likes\"].sum()/corr_train[\"Num of Profile Likes\"].sum())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UipdKn8ggwan"
      },
      "source": [
        "plt.imshow(X=bool_ar,cmap='gray')\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xwkJ6TYygwan"
      },
      "source": [
        "print(pixel_index_selection)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6HIOvAI7gwan"
      },
      "source": [
        "len(df_train_img.columns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_WOM9-Igwan"
      },
      "source": [
        "df_img_test = pd.DataFrame()\r\n",
        "pixel_index_selection = [2043, 1947, 2046, 2142, 1893, 2139, 1986, 2238, 1944, 2079, 1950, 2337, 1848, 1752, 1509, 1797, 1983, 2181, 2040, 2082, 1413, 1989, \r\n",
        "                            1851, 2274, 1656, 1566, 2241, 1605, 1896, 1992, 1701, 2178, 1599, 2268, 2235, 1695, 2271, 1560, 2175, 1890, 2364, 2085, 1293, 2367, \r\n",
        "                            1662, 1290, 1287, 1464, 1857, 1414, 1296, 1488, 2370, 525, 1506, 1368, 2340, 1284, 1761, 1299, 1884, 1485, 1758, 1677, 1791, 1117, \r\n",
        "                            1680, 1302, 2334, 2145, 1305, 1779, 1389, 1212, 1308, 1584, 1773, 1776, 1114, 1392, 1788, 1899, 2160, 1770, 1860, 2157, 1881, 1206, \r\n",
        "                            1581, 1395, 1872, 1386, 1869, 1767, 1749, 2361, 2344, 1473, 1203]\r\n",
        "  \r\n",
        "selected_color_pixels = np.array([True if (i in pixel_index_selection) else False for i in range(0,len(df_train_img.columns)-1)])\r\n",
        "  \r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8R7UGPIAgwan"
      },
      "source": [
        "col_names = ['pixel_' + str(i) for i in sorted(pixel_index_selection)]\r\n",
        "print(col_names)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xgWatYzPgwan"
      },
      "source": [
        "df_img_test = pd.DataFrame(np.array(img_train).reshape([len(img_train),-1])).loc[:,selected_color_pixels]\r\n",
        "df_img_test.columns = col_names"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WNBNoAbQgwan"
      },
      "source": [
        "df_img_test['Sum Selected Pixels'] = df_img_test[col_names].sum(axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I70RlBFKgwao"
      },
      "source": [
        "np.sum(df_img_test.loc[0,:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w-R39zIxgwao"
      },
      "source": [
        "df_img_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rtPOAT_cgwao"
      },
      "source": [
        "corr_train.head(top).index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ee9Ou-eFgwao"
      },
      "source": [
        "corr_sum = []\r\n",
        "tot_corr = corr_train[\"Num of Profile Likes\"].sum()\r\n",
        "for top in range(1,1000):\r\n",
        "  bool_ar = np.array([255 if (i in corr_train.head(top).index) else 0 for i in range(0,len(df_train_img.columns)-1)])\r\n",
        "  bool_ar = bool_ar.reshape(32,32,3)\r\n",
        "  corr_sum.append(corr_train.head(top)[\"Num of Profile Likes\"].sum()/tot_corr)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9VgOJp9vgwao"
      },
      "source": [
        "plt.plot(range(1,1000),corr_sum)\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W8ONm_Xqgwao"
      },
      "source": [
        "plt.plot(range(1,3073),corr_train[\"Num of Profile Likes\"][1:3073])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YnZwgndHgwap"
      },
      "source": [
        "corr_train[\"Num of Profile Likes\"][1:3070]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y8adv4z9gwap"
      },
      "source": [
        "df_train_img.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g_ngb9QOgwap"
      },
      "source": [
        "df_clust_train = pd.DataFrame()\r\n",
        "df_clust_test = pd.DataFrame()\r\n",
        "\r\n",
        "#Neural Network for image feature extraction - didn't show better results. Maybe to explore further.\r\n",
        "''' images  = np.array(np.float32(img_train).reshape(len(img_train), -1)/255)\r\n",
        "model = tf.keras.applications.MobileNetV2(include_top=False,weights='imagenet', input_shape=(32, 32, 3))\r\n",
        "predictions = model.predict(images.reshape(-1, 32, 32, 3))\r\n",
        "pred_images = predictions.reshape(images.shape[0], -1) '''\r\n",
        "\r\n",
        "df_train_img = pd.DataFrame(np.array(img_train).reshape([len(img_train),-1]))\r\n",
        "df_test_img = pd.DataFrame(np.array(img_test).reshape([len(img_test),-1]))\r\n",
        "\r\n",
        "#PCA can also be applied prior to clustering\r\n",
        "pca = PCA(n_components=5)\r\n",
        "df_train_img_pca = pca.fit(df_train_img).transform(df_train_img)\r\n",
        "df_test_img_pca = pca.transform(df_test_img)\r\n",
        "\r\n",
        "#Spectral Clustering\r\n",
        "kspec = SpectralClustering(n_clusters=20,random_state=1,n_neighbors=5,affinity='nearest_neighbors')\r\n",
        "df_clust_train['Spectral Clusters'] = kspec.fit_predict(df_train_img_pca)\r\n",
        "df_clust_test['Spectral Clusters'] = kspec.fit_predict(df_test_img_pca)\r\n",
        "\r\n",
        "#K-Means Clustering\r\n",
        "kmeans = KMeans(n_clusters=20,random_state=1)\r\n",
        "df_clust_train['KMeans Clusters'] = pd.DataFrame(kmeans.fit_predict(df_train_img_pca))\r\n",
        "df_clust_test['KMeans Clusters'] = pd.DataFrame(kmeans.fit_predict(df_test_img_pca))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMFIatfNgwap"
      },
      "source": [
        "df_clust_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v6ZyE8Dtgwap"
      },
      "source": [
        "## Preprocessing Functions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQDjnosngwap"
      },
      "source": [
        "### Tidying Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7WaQLKLBgwap"
      },
      "source": [
        "def tidyer(dataset_train, dataset_test):\n",
        "  tidy_dataset_list = []\n",
        "  tidy_dataset = pd.DataFrame()\n",
        "\n",
        "  #Time zone & UTC Offset mapping table for missing values\n",
        "  merge_df_time = pd.concat([dataset_train[['UTC Offset','User Time Zone']],dataset_test[['UTC Offset','User Time Zone']]], axis=0)    #mapping table\n",
        "  tz_mapping = merge_df_time.pivot_table(index=['User Time Zone','UTC Offset'],values='UTC Offset',aggfunc='count').reset_index(drop=False)    #mapping table\n",
        "  utc_labels = ['America','Europe & Africa','Middle East','East Asia']  \n",
        "\n",
        "  #Num of profiles likes only in traning set\n",
        "  tidy_dataset['Likes'] = np.log(1+dataset_train['Num of Profile Likes'])\n",
        "  \n",
        "  for dataset in [dataset_train,dataset_test]:\n",
        "    tidy_dataset['Name length'] = dataset['User Name'].apply(lambda x:len(x))\n",
        "    tidy_dataset['URL bool'] = dataset['Personal URL'].str.len() > 12\n",
        "\n",
        "    tidy_dataset['Cover Image Status'] = dataset['Profile Cover Image Status'].copy()\n",
        "    tidy_dataset['Cover Image Status'][(tidy_dataset['Cover Image Status'] != 'Set') & (tidy_dataset['Cover Image Status'] != 'Not set')] = 'Unknown'\n",
        "\n",
        "    tidy_dataset['Profile Verification Status'] = dataset['Profile Verification Status'].copy()\n",
        "\n",
        "    #old method to convert color features to RGB colors\n",
        "    ''' temp_ds = temp_fea = dataset[['Profile Text Color','Profile Page Color','Profile Theme Color']].applymap(lambda x:'000000' if (pd.isnull(x) | (len(str(x)) != 6)) else x)\n",
        "    reds,greens,blues = ['Text Red','Page Red','Theme Red'],['Text Green','Page Green','Theme Green'],['Text Blue','Page Blue','Theme Blue']\n",
        "    tidy_dataset[reds],tidy_dataset[greens],tidy_dataset[blues] = temp_ds.applymap(lambda x:int(x[0:2],16)),temp_ds.applymap(lambda x:int(x[2:4],16)),temp_ds.applymap(lambda x:int(x[4:6],16))\n",
        "    '''\n",
        "\n",
        "    temp_ds = dataset[['Profile Text Color','Profile Page Color','Profile Theme Color']].applymap(lambda x:'000000' if (pd.isnull(x) | (len(str(x)) != 6)) else x)\n",
        "    tidy_dataset[['Profile Text Color bool','Profile Page Color bool','Profile Theme Color bool']] = temp_ds[['Profile Text Color',\n",
        "                                                                                                          'Profile Page Color',\n",
        "                                                                                                          'Profile Theme Color']].applymap(lambda x:False if x==\"000000\" else True)\n",
        "\n",
        "    tidy_dataset['View Customized bool'] = dataset['Is Profile View Size Customized?'].copy()\n",
        "\n",
        "    #tz_mapping = dataset[['UTC Offset','User Time Zone']].pivot_table(index=['User Time Zone','UTC Offset'],values='UTC Offset',aggfunc='count').reset_index(drop=False)    #mapping table\n",
        "            \n",
        "    tidy_dataset['Time Region'] = dataset['User Time Zone'].apply(lambda x:utc_labels[np.digitize(tz_mapping[tz_mapping['User Time Zone']==x]['UTC Offset'].values[0],[-5400,9000,23400])] if not pd.isnull(x) else 'Unknown')\n",
        "    \n",
        "    tidy_dataset['Location Public Visibility'] = dataset['Location Public Visibility'].apply(lambda x:x.lower() if not pd.isnull(x) else 'unknown')\n",
        "    tidy_dataset['User Language'] =  dataset['User Language'].apply(lambda x:langcodes.Language.get(x).describe()['language'])\n",
        "    tidy_dataset['Profile Creation Year'] = dataset['Profile Creation Timestamp'].apply(lambda x:int(x[-4:]))\n",
        "\n",
        "    tidy_dataset['Num of Followers'] = np.log(1+dataset['Num of Followers'].fillna(value=dataset['Num of Followers'].median()))\n",
        "    tidy_dataset['Num of People Following'] = np.log(1+dataset['Num of People Following'].fillna(value=dataset['Num of People Following'].median()))\n",
        "    tidy_dataset['Num of Status Updates'] = np.log(1+dataset['Num of Status Updates'].fillna(value=dataset['Num of Status Updates'].median()))\n",
        "    tidy_dataset['Num of Direct Messages'] = np.log(1+dataset['Num of Direct Messages'].fillna(value=dataset['Num of Direct Messages'].median()))\n",
        "    tidy_dataset['Avg Daily Profile Visit Duration in seconds'] = dataset['Avg Daily Profile Visit Duration in seconds'].fillna(value=dataset['Avg Daily Profile Visit Duration in seconds'].median())\n",
        "    tidy_dataset['Avg Daily Profile Clicks'] = np.log(1+dataset['Avg Daily Profile Clicks'].fillna(value=dataset['Avg Daily Profile Clicks'].median()))\n",
        "\n",
        "    tidy_dataset['Profile Category'] = dataset['Profile Category'].apply(lambda x:x.lower() if (not pd.isnull(x) | (x==' ')) else 'unknown')\n",
        "\n",
        "    tidy_dataset_list.append(tidy_dataset)\n",
        "    tidy_dataset = pd.DataFrame()           #reinitialize DF for test set\n",
        "\n",
        "  return tidy_dataset_list[0],tidy_dataset_list[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "loES4Il6gwaq"
      },
      "source": [
        "### Image Feature Extraction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2xsJg5xJgwaq"
      },
      "source": [
        "def img_extr(img_array,pixel_index_selection):\r\n",
        "\r\n",
        "  df_img = pd.DataFrame()\r\n",
        "\r\n",
        "  #Pixel selection based on correlation analysis\r\n",
        "  col_names = ['pixel_' + str(i) for i in sorted(pixel_index_selection)]\r\n",
        "  selected_color_pixels = np.array([True if (i in pixel_index_selection) else False for i in range(0,3072)])\r\n",
        "  \r\n",
        "  df_img = pd.DataFrame(np.array(img_array).reshape([len(img_array),-1])).loc[:,selected_color_pixels]\r\n",
        "  df_img.columns = col_names\r\n",
        "\r\n",
        "  #Sum of selected pixel values\r\n",
        "  df_img['Sum Selected Pixels'] = df_img.sum(axis=1)\r\n",
        "\r\n",
        "  #Number of black pixels & average colors exploration\r\n",
        "  temp_fea = []\r\n",
        "\r\n",
        "  for image in img_array:\r\n",
        "    black_one = 0\r\n",
        "    black_zero = 0\r\n",
        "    red = 0\r\n",
        "    green = 0\r\n",
        "    blue = 0\r\n",
        "    for line in image:\r\n",
        "      for pixel in line:\r\n",
        "        red += pixel[0]\r\n",
        "        green += pixel[1]\r\n",
        "        blue += pixel[2]\r\n",
        "        if list(pixel) == [1,1,1]:\r\n",
        "          black_one += 1\r\n",
        "        elif list(pixel) == [0,0,0]:\r\n",
        "          black_zero += 1\r\n",
        "    temp_fea.append([black_zero,black_one,red/(32*32),green/(32*32),blue/(32*32)])\r\n",
        "  temp_fea = pd.DataFrame(temp_fea,columns=['black_zero','black_one','red','green','blue'])\r\n",
        "\r\n",
        "  df_img['Img Black Zeros bool'] = temp_fea['black_zero'] > temp_fea['black_one']\r\n",
        "  df_img['Img Num Black Pixels'] = temp_fea['black_zero'] + temp_fea['black_one']\r\n",
        "  df_img['Img Average Red level'],df_img['Img Average Green level'],df_img['Img Average Blue level'] = temp_fea['red'],temp_fea['green'],temp_fea['blue']\r\n",
        "\r\n",
        "  return df_img"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lRq--Qucgwaq"
      },
      "source": [
        "def img_clust(img_array_train,img_array_test):\r\n",
        "\r\n",
        "  df_clust_train,df_clust_test = pd.DataFrame(),pd.DataFrame()\r\n",
        "\r\n",
        "  #Neural Network for image feature extraction - didn't show better results. Maybe to explore further.\r\n",
        "  ''' images  = np.array(np.float32(img_train).reshape(len(img_train), -1)/255)\r\n",
        "  model = tf.keras.applications.MobileNetV2(include_top=False,weights='imagenet', input_shape=(32, 32, 3))\r\n",
        "  predictions = model.predict(images.reshape(-1, 32, 32, 3))\r\n",
        "  pred_images = predictions.reshape(images.shape[0], -1) '''\r\n",
        "\r\n",
        "  df_train_img = pd.DataFrame(np.array(img_array_train).reshape([len(img_array_train),-1]))\r\n",
        "  df_test_img = pd.DataFrame(np.array(img_array_test).reshape([len(img_array_test),-1]))\r\n",
        "\r\n",
        "  #PCA can also be applied prior to clustering\r\n",
        "  pca = PCA(n_components=5,random_state=42)\r\n",
        "  df_train_img_pca = pca.fit(df_train_img).transform(df_train_img)\r\n",
        "  df_test_img_pca = pca.transform(df_test_img)\r\n",
        "\r\n",
        "  #Spectral Clustering\r\n",
        "  kspec = SpectralClustering(n_clusters=20,random_state=42,n_neighbors=5,affinity='nearest_neighbors')\r\n",
        "  df_clust_train['Img Spectral Clusters'] = kspec.fit_predict(df_train_img_pca).astype(np.str)\r\n",
        "  df_clust_test['Img Spectral Clusters'] = kspec.fit_predict(df_test_img_pca).astype(np.str)\r\n",
        "\r\n",
        "  #K-Means Clustering\r\n",
        "  kmeans = KMeans(n_clusters=20,random_state=42)\r\n",
        "  df_clust_train['Img KMeans Clusters'] = pd.DataFrame(kmeans.fit_predict(df_train_img_pca).astype(np.str))\r\n",
        "  df_clust_test['Img KMeans Clusters'] = pd.DataFrame(kmeans.predict(df_test_img_pca).astype(np.str))\r\n",
        "  \r\n",
        "  return df_clust_train,df_clust_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dd5LHzq1gwaq"
      },
      "source": [
        "### Feature Standardization & Encoding Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3EBLSH7agwaq"
      },
      "source": [
        "def standard_encoder(dataset,test_data, num_fea,cat_fea):\n",
        "  standard_dataset = dataset.copy()\n",
        "  test_dataset = test_data.copy()\n",
        "\n",
        "  for feature in cat_fea:\n",
        "    lb = LabelBinarizer()\n",
        "    temp_ds = lb.fit_transform(standard_dataset[feature])\n",
        "    standard_dataset = pd.concat([standard_dataset,pd.DataFrame(temp_ds,columns=[feature + ' ' + label for label in lb.classes_])],axis=1).drop([feature], axis=1)\n",
        "    temp_ds = lb.transform(test_dataset[feature])\n",
        "    test_dataset = pd.concat([test_dataset, pd.DataFrame(temp_ds,columns=[feature + ' ' + label for label in lb.classes_])],axis=1).drop([feature], axis=1) \n",
        "\n",
        "\n",
        "  for feature in num_fea:\n",
        "    sc = StandardScaler()\n",
        "    standard_dataset[feature] = sc.fit_transform(np.array(standard_dataset[feature]).reshape(-1,1))\n",
        "    test_dataset[feature] = sc.transform(np.array(test_dataset[feature]).reshape(-1,1))\n",
        "  # test_dataset.head()\n",
        "  \n",
        "  return standard_dataset, test_dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7zaPIcAgwaq"
      },
      "source": [
        "### Feature Selection Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8UOaFG-1gwaq"
      },
      "source": [
        "def feature_select(dataset_train):\r\n",
        "  corr_prep = dataset_train.corr().abs().sort_values(by=[\"Likes\"],ascending=False)\r\n",
        "\r\n",
        "  return corr_prep"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_oFMfkj5gwar"
      },
      "source": [
        "## Further Data Exploration and Model Exploration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0vOQ4kEggwar"
      },
      "source": [
        "### Tidy set Visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LmqbFbbmgwar"
      },
      "source": [
        "tidy_df_train,tidy_df_test = tidyer(df_train,df_test)\n",
        "#tidy_df_test = tidyer(df_test,False)\n",
        "\n",
        "#pixel selection\n",
        "pixel_index_selection = [2043, 1947, 2046, 2142, 1893, 2139, 1986, 2238, 1944, 2079, 1950, 2337, 1848, 1752, 1509, 1797, 1983, 2181, 2040, 2082, 1413, 1989, \n",
        "                            1851, 2274, 1656, 1566, 2241, 1605, 1896, 1992, 1701, 2178, 1599, 2268, 2235, 1695, 2271, 1560, 2175, 1890, 2364, 2085, 1293, 2367, \n",
        "                            1662, 1290, 1287, 1464, 1857, 1414, 1296, 1488, 2370, 525, 1506, 1368, 2340, 1284, 1761, 1299, 1884, 1485, 1758, 1677, 1791, 1117, \n",
        "                            1680, 1302, 2334, 2145, 1305, 1779, 1389, 1212, 1308, 1584, 1773, 1776, 1114, 1392, 1788, 1899, 2160, 1770, 1860, 2157, 1881, 1206, \n",
        "                            1581, 1395, 1872, 1386, 1869, 1767, 1749, 2361, 2344, 1473, 1203]\n",
        "\n",
        "tidy_df_train = pd.concat([tidy_df_train,img_extr(img_train,pixel_index_selection)],axis=1)\n",
        "tidy_df_test = pd.concat([tidy_df_test,img_extr(img_test,pixel_index_selection)],axis=1)\n",
        "img_clust_train,img_clust_test = img_clust(img_train,img_test)\n",
        "tidy_df_train,tidy_df_test = pd.concat([tidy_df_train,img_clust_train],axis=1),pd.concat([tidy_df_test,img_clust_test],axis=1)\n",
        "\n",
        "\n",
        "num_fea = ['Name length','Profile Creation Year','Num of Followers','Num of People Following','Num of Status Updates','Num of Direct Messages','Avg Daily Profile Visit Duration in seconds',\n",
        "           'Avg Daily Profile Clicks','Img Num Black Pixels', 'Img Average Red level',\t'Img Average Green level',\t'Img Average Blue level',\n",
        "           'Sum Selected Pixels'] + ['pixel_' + str(i) for i in sorted(pixel_index_selection)]\n",
        "cat_fea = ['Cover Image Status','Profile Verification Status','Time Region','Location Public Visibility','User Language','Profile Category','Img Spectral Clusters','Img KMeans Clusters']\n",
        "prep_df_train,prep_df_test = standard_encoder(tidy_df_train,tidy_df_test,num_fea,cat_fea)\n",
        "#prep_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3PghxlJAgwar"
      },
      "source": [
        "tidy_df_train.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3aTrTtSEgwar"
      },
      "source": [
        "prep_df_train.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "00QBWud_gwar"
      },
      "source": [
        "prep_df_test.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iRcNgNhcgwar"
      },
      "source": [
        "sns.boxplot(x='URL bool',y='Likes',data=tidy_df_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5DjuwccDgwar"
      },
      "source": [
        "num_fea = ['Num of Followers','Num of People Following','Num of Status Updates','Num of Direct Messages','Avg Daily Profile Visit Duration in seconds','Avg Daily Profile Clicks','Likes']\n",
        "sns.pairplot(tidy_df_train,x_vars=num_fea,y_vars=num_fea,hue='URL bool')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KqgNumR-gwas"
      },
      "source": [
        "num_fea = ['Name length','Text Red','Page Red','Theme Red','Text Green','Page Green','Theme Green','Text Blue','Page Blue','Theme Blue','Profile Creation Year','Likes']\n",
        "sns.pairplot(tidy_df_train,x_vars=num_fea,y_vars=num_fea,hue='Profile Verification Status')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5wbcUfX-gwas"
      },
      "source": [
        "Most of numerical features does not show good correlation with number fo Likes. The Number of People Following and the Average Profile Clicks are the one that have a little correlation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5zVKDfwpgwas"
      },
      "source": [
        "prep_df_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wLYsf1Gugwas"
      },
      "source": [
        "### Feature Selection Exploration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6NTR0YBqgwas"
      },
      "source": [
        "corr_prep = prep_df_train.corr().abs().sort_values(by=[\"Likes\"],ascending=False)\r\n",
        "for i in range(0,len(corr_prep.index)):\r\n",
        "  print(i,': ', corr_prep.index[i],' -> ', corr_prep['Likes'][i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0FFQlSQMgwas"
      },
      "source": [
        "for i in prep_df_train.columns:\r\n",
        "  print(i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VKbBsAXUgwas"
      },
      "source": [
        "corr_prep['Likes'][0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WGRANSyDgwas"
      },
      "source": [
        "fea_sel_mod = SelectKBest(score_func=mutual_info_regression, k=50)\r\n",
        "fea_sel_mod.fit(prep_df_train.iloc[:,1:203],prep_df_train.iloc[:,0])\r\n",
        "fea_sel_prep = pd.DataFrame({'Feature names':prep_df_train.iloc[:,1:203].columns,'Feature selection score':fea_sel_mod.scores_}).sort_values(by=[\"Feature selection score\"],ascending=False).reset_index(drop=True)\r\n",
        "for i in range(0,len(fea_sel_prep.index)):\r\n",
        "  print(i,': ', fea_sel_prep['Feature names'][i],' -> ', fea_sel_prep['Feature selection score'][i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_kBlg3-gwat"
      },
      "source": [
        "### Model Exploration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3LbPHg_hgwat"
      },
      "source": [
        "from sklearn.metrics import mean_squared_log_error\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.ensemble import ExtraTreesRegressor\n",
        "from sklearn.ensemble import AdaBoostRegressor\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import (\n",
        "    RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor, \n",
        "    VotingRegressor, StackingRegressor, BaggingRegressor,\n",
        ")\n",
        "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
        "from sklearn.metrics import make_scorer\n",
        "from sklearn import preprocessing\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.svm import SVR, LinearSVR\n",
        "from xgboost import XGBRegressor\n",
        "import seaborn as sns\n",
        "from  matplotlib import pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn import linear_model\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import RobustScaler, MinMaxScaler\n",
        "from scipy.special import boxcox1p, inv_boxcox1p\n",
        "from scipy.stats import boxcox_normmax\n",
        "from scipy.stats import skew, norm\n",
        "from scipy.stats import yeojohnson\n",
        "from sklearn.preprocessing import PowerTransformer\n",
        "from lightgbm import LGBMRegressor\n",
        "from catboost import CatBoostRegressor\n",
        "from sklearn.linear_model import RidgeCV\n",
        "\n",
        "#Correlration based\n",
        "all_features = corr_prep.index\n",
        "all_features = all_features[1:]\n",
        "\n",
        "#all_features = fea_sel_prep['Feature names']\n",
        "\n",
        "#Top correlated features\n",
        "#cols_x = ['URL bool','Num of People Following','Num of Status Updates','Location Public Visibility disabled','Location Public Visibility enabled','Profile Category unknown','Avg Daily Profile Clicks','Profile Verification Status Verified','Profile Verification Status Not verified','Sum Selected Pixels']\n",
        "cols_x = all_features[0:35]\n",
        "cols_y = 'Likes'\n",
        "\n",
        "#All X features\n",
        "#.iloc[:,1:65]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(prep_df_train[cols_x], prep_df_train[cols_y], \n",
        "                                                    test_size=0.3, \n",
        "                                                    shuffle=True, \n",
        "                                                    random_state=76 #  To guarantee that the split will always be the same\n",
        "                                                    )  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dP99UNS9gwat"
      },
      "source": [
        "result = pd.DataFrame(columns = (\"Classifiers\",\"Training\",\"Testing\"))\n",
        "classifier, train_scores, test_scores = [],[],[]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CjPMCyf8gwat"
      },
      "source": [
        "clf_tree = DecisionTreeRegressor(max_depth=10)\n",
        "clf_tree = clf_tree.fit(X_train,y_train)\n",
        "classifier.append(\"DecisionTreeRegressor\")\n",
        "train_scores.append(np.sqrt(mean_squared_log_error(np.exp(y_train), np.exp(clf_tree.predict(X_train)))))\n",
        "test_scores.append(np.sqrt(mean_squared_log_error(np.exp(y_test), np.exp(clf_tree.predict(X_test)))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iKToqMZSgwat"
      },
      "source": [
        "clf_lin = LinearRegression()\n",
        "clf_lin = clf_lin.fit(X_train,y_train)\n",
        "classifier.append(\"LinearRegression\")\n",
        "train_scores.append(np.sqrt(mean_squared_log_error(np.exp(y_train), np.exp(clf_lin.predict(X_train)))))\n",
        "test_scores.append(np.sqrt(mean_squared_log_error(np.exp(y_test), np.exp(clf_lin.predict(X_test)))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q4HsN3Ptgwat"
      },
      "source": [
        "knn = KNeighborsRegressor(n_neighbors = 3)\n",
        "knn.fit(X_train, y_train)\n",
        "classifier.append(\"KNeighborsRegressor\")\n",
        "train_scores.append(np.sqrt(mean_squared_log_error(np.exp(y_train), np.exp(knn.predict(X_train)))))\n",
        "test_scores.append(np.sqrt(mean_squared_log_error(np.exp(y_test), np.exp(knn.predict(X_test)))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vhZ_n0yxgwat"
      },
      "source": [
        "random_forest = RandomForestRegressor(n_estimators=100)\n",
        "random_forest.fit(X_train, y_train)\n",
        "classifier.append(\"RandomForestRegressor\")\n",
        "train_scores.append(np.sqrt(mean_squared_log_error(np.exp(y_train), np.exp(random_forest.predict(X_train)))))\n",
        "test_scores.append(np.sqrt(mean_squared_log_error(np.exp(y_test), np.exp(random_forest.predict(X_test)))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JXvjUXfogwat"
      },
      "source": [
        "clf_ext = ExtraTreesRegressor(\n",
        "    max_features='auto',\n",
        "    bootstrap=True,\n",
        "    oob_score=True,\n",
        "    #n_estimators=1000,\n",
        "    #max_depth=None,\n",
        "    max_depth = 10,\n",
        "    min_samples_leaf = 10,\n",
        "    n_estimators = 2000,\n",
        "    #min_samples_split=10\n",
        "    min_samples_split = 20\n",
        "    #class_weight=\"balanced\",\n",
        "    #min_weight_fraction_leaf=0.02\n",
        "    )\n",
        "clf_ext = clf_ext.fit(X_train,y_train)\n",
        "classifier.append(\"ExtraTreesRegressor\")\n",
        "train_scores.append(np.sqrt(mean_squared_log_error(np.exp(y_train), np.exp(clf_ext.predict(X_train)))))\n",
        "test_scores.append(np.sqrt(mean_squared_log_error(np.exp(y_test), np.exp(clf_ext.predict(X_test)))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AYGGZtYLgwat"
      },
      "source": [
        "clf_ada = AdaBoostRegressor(n_estimators=400, learning_rate=0.1)\n",
        "clf_ada.fit(X_train,y_train)\n",
        "classifier.append(\"AdaBoostRegressor\")\n",
        "train_scores.append(np.sqrt(mean_squared_log_error(np.exp(y_train), np.exp(clf_ada.predict(X_train)))))\n",
        "test_scores.append(np.sqrt(mean_squared_log_error(np.exp(y_test), np.exp(clf_ada.predict(X_test)))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AFN7rb4Hgwat"
      },
      "source": [
        "#Stacking\n",
        "m_rf = RandomForestRegressor(n_estimators=200,\n",
        "                              max_depth=2,\n",
        "                              min_samples_split=8,\n",
        "                              min_samples_leaf=8,\n",
        "                              max_features=None,\n",
        "                              oob_score=True,\n",
        "                              random_state=42)\n",
        "\n",
        "m_svr = SVR(C=20, epsilon=0.004, gamma=0.0001)\n",
        "\n",
        "m_ab = AdaBoostRegressor(base_estimator=DecisionTreeRegressor(max_depth=5),\n",
        "                        n_estimators=200,\n",
        "                        loss='exponential',\n",
        "                        learning_rate=0.01,random_state=42)\n",
        "m_bag = BaggingRegressor(n_estimators=15,max_samples=10,max_features=5,random_state=42)\n",
        "\n",
        "xgboost = XGBRegressor(learning_rate=0.01,\n",
        "                      n_estimators=1000,\n",
        "                      max_depth=5,\n",
        "                      gamma=0.3,\n",
        "                      subsample=0.9,\n",
        "                      colsample_bytree=0.8,\n",
        "                      objective='reg:squarederror',\n",
        "                      nthread=-1,\n",
        "                      scale_pos_weight=1,\n",
        "                      seed=27,\n",
        "                      reg_alpha=0.00009,\n",
        "                      random_state=42)\n",
        "m_gbr = GradientBoostingRegressor(n_estimators=500,\n",
        "                              learning_rate=0.03,\n",
        "                              max_depth=4,\n",
        "                              max_features='sqrt',\n",
        "                              min_samples_leaf=30,\n",
        "                              min_samples_split=20,\n",
        "                              loss='ls',\n",
        "                              random_state=42)\n",
        "m_catboost = CatBoostRegressor(n_estimators=1000,\n",
        "                              learning_rate=0.01,\n",
        "                              rsm=0.4,\n",
        "                              random_state=42)\n",
        "model = StackingRegressor(\n",
        "      estimators=[\n",
        "          ('m_rf', m_rf),\n",
        "          # ('m_ridge', m_ridge),\n",
        "          ('m_svr', m_svr),\n",
        "          ('m_bag', m_bag),\n",
        "          # ('m_ab', m_ab),\n",
        "          ('m_gbr', m_gbr),\n",
        "          ('xgboost', xgboost),\n",
        "          # ('m_catboost', m_catboost),\n",
        "      ],\n",
        "      n_jobs=-1,\n",
        "      verbose=0.5)\n",
        "\n",
        "model.fit(X_train,y_train)\n",
        "classifier.append(\"Stack\")\n",
        "train_scores.append(np.sqrt(mean_squared_log_error(np.exp(y_train), np.exp(model.predict(X_train)))))\n",
        "test_scores.append(np.sqrt(mean_squared_log_error(np.exp(y_test), np.exp(model.predict(X_test)))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fGG5p39ggwat"
      },
      "source": [
        "#,for i,text in enumerate(classifier):\n",
        "    #result.loc[i+1] = [text,np.around(train_scores[i], decimals = 2),np.around(test_scores[i], decimals=2)]\n",
        "print(train_scores,'\\n\\n',test_scores)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qRqMbkPpgwau"
      },
      "source": [
        "np.exp(y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6dEGhVypgwau"
      },
      "source": [
        "np.exp(clf_tree.predict(X_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dum5WTUXgwau"
      },
      "source": [
        "m_rf = RandomForestRegressor(n_estimators=200,\r\n",
        "                              max_depth=2,\r\n",
        "                              min_samples_split=8,\r\n",
        "                              min_samples_leaf=8,\r\n",
        "                              max_features=None,\r\n",
        "                              oob_score=True,\r\n",
        "                              random_state=42)\r\n",
        "\r\n",
        "kf = KFold(n_splits=5, random_state=42, shuffle=True)\r\n",
        "ridge_alphas = [1e-15, 1e-10, 1e-8, 9e-4, 7e-4, 5e-4, 3e-4, 1e-4, 1e-3, 5e-2, \r\n",
        "              1e-2, 0.1, 0.3, 1, 3, 5, 10, 15, 18, 50, 75, 100]\r\n",
        "m_ridge = RidgeCV(alphas=ridge_alphas, cv=kf)\r\n",
        "m_svr = SVR(C=20, epsilon=0.004, gamma=0.0001)\r\n",
        "\r\n",
        "# params = {'n_estimators': 500,\r\n",
        "#           'max_depth': 4,\r\n",
        "#           'min_samples_split': 5,\r\n",
        "#           'learning_rate': 0.01,\r\n",
        "#           'loss': 'ls'}\r\n",
        "# m_gbr = GradientBoostingRegressor(**params)\r\n",
        "m_ab = AdaBoostRegressor(base_estimator=DecisionTreeRegressor(max_depth=5),\r\n",
        "                        n_estimators=200,\r\n",
        "                        loss='exponential',\r\n",
        "                        learning_rate=0.01,random_state=42)\r\n",
        "m_bag = BaggingRegressor(n_estimators=15,max_samples=10,max_features=5,random_state=42)\r\n",
        "# m_knn = KNeighborsRegressor()\r\n",
        "# m_svr = SVR()\r\n",
        "# model = VotingRegressor(\r\n",
        "#     estimators=[('rf', m_rf), ('ridge', m_ridge)], \r\n",
        "#     n_jobs=-1)\r\n",
        "xgboost = XGBRegressor(learning_rate=0.01,\r\n",
        "                      n_estimators=1000,\r\n",
        "                      max_depth=5,\r\n",
        "                      gamma=0.3,\r\n",
        "                      subsample=0.9,\r\n",
        "                      colsample_bytree=0.8,\r\n",
        "                      objective='reg:squarederror',\r\n",
        "                      nthread=-1,\r\n",
        "                      scale_pos_weight=1,\r\n",
        "                      seed=27,\r\n",
        "                      reg_alpha=0.00009,\r\n",
        "                      random_state=42)\r\n",
        "m_gbr = GradientBoostingRegressor(n_estimators=500,\r\n",
        "                              learning_rate=0.03,\r\n",
        "                              max_depth=4,\r\n",
        "                              max_features='sqrt',\r\n",
        "                              min_samples_leaf=30,\r\n",
        "                              min_samples_split=20,\r\n",
        "                              loss='ls',\r\n",
        "                              random_state=42)\r\n",
        "m_catboost = CatBoostRegressor(n_estimators=1000,\r\n",
        "                              learning_rate=0.01,\r\n",
        "                              rsm=0.4,\r\n",
        "                              random_state=42)\r\n",
        "model = StackingRegressor(\r\n",
        "      estimators=[\r\n",
        "          ('m_rf', m_rf),\r\n",
        "          # ('m_ridge', m_ridge),\r\n",
        "          ('m_svr', m_svr),\r\n",
        "          ('m_bag', m_bag),\r\n",
        "          # ('m_ab', m_ab),\r\n",
        "          ('m_gbr', m_gbr),\r\n",
        "          ('xgboost', xgboost),\r\n",
        "          # ('m_catboost', m_catboost),\r\n",
        "      ],\r\n",
        "      n_jobs=-1,\r\n",
        "      verbose=0.5)\r\n",
        "\r\n",
        "cv = KFold(n_splits=5, random_state=42, shuffle=True)\r\n",
        "pred = cross_val_predict(model, X_train, y_train, cv=cv)\r\n",
        "pred = np.abs(pred) # Take abs for negative values due to RMSLE metric\r\n",
        "#print(rmsle(np.expm1(y_train), np.expm1(pred)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yHgLPHcIgwau"
      },
      "source": [
        "pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MXaDbiQIgwau"
      },
      "source": [
        "print(np.sqrt(mean_squared_log_error(np.expm1(y_train), np.expm1(pred))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nNTvmvrkgwau"
      },
      "source": [
        "#Grid Search\r\n",
        "from sklearn.model_selection import GridSearchCV\r\n",
        "from sklearn.model_selection import KFold\r\n",
        "from sklearn.metrics import classification_report\r\n",
        "from sklearn.svm import SVR\r\n",
        "from sklearn.model_selection import RandomizedSearchCV \r\n",
        "\r\n",
        "from sklearn.cluster import SpectralClustering\r\n",
        "\r\n",
        "from sklearn.model_selection import cross_val_score\r\n",
        "\r\n",
        "# Set the parameters by cross-validation\r\n",
        "''' tuned_parameters = [\r\n",
        "  {'m_rf__n_estimators': [50, 200, 400],\r\n",
        "  'm_rf__max_depth': [2, 4, 7],\r\n",
        "  'm_rf__min_samples_split': [3, 5, 8],\r\n",
        "  'm_rf__max_features': [3, 5, 8]},\r\n",
        "\r\n",
        "  #RandomForestRegressor(n_estimators=200,\r\n",
        "                              #max_depth=4,\r\n",
        "                              #min_samples_split=5,\r\n",
        "                              #min_samples_leaf=5,\r\n",
        "                              #max_features=None,\r\n",
        "                              #oob_score=True,\r\n",
        "                              #random_state=42)\r\n",
        "\r\n",
        "  {'m_svr__epsilon': [0.004, 0.008, 0.0016],\r\n",
        "   'm_svr__C': [10, 20, 30],\r\n",
        "   'm_svr__gamma': [0.0001, 0.0003, 0.0007]},\r\n",
        "\r\n",
        "   #m_svr = SVR(C=20, epsilon=0.008, gamma=0.0003)\r\n",
        "\r\n",
        "  {'m_bag__n_estimators': [5, 10, 15],\r\n",
        "   'm_bag__max_samples': [1, 5, 10],\r\n",
        "   'm_bag__max_features': [1, 5, 10]},\r\n",
        "\r\n",
        "  #m_bag = BaggingRegressor()\r\n",
        "\r\n",
        "  {'m_gbr__n_estimators': [250, 500, 750],\r\n",
        "   'm_gbr__learning_rate': [0.01, 0.03, 0.05],\r\n",
        "   'm_gbr__max_features': ['auto', 'sqrt', 'log2'],\r\n",
        "   'm_gbr__min_samples_leaf': [7, 15, 30],\r\n",
        "   'm_gbr__min_samples_split': [7, 10, 20],\r\n",
        "   'm_gbr__loss': ['huber', 'ls', 'lad']},\r\n",
        "\r\n",
        "  #m_gbr = GradientBoostingRegressor(n_estimators=500,\r\n",
        "                              #learning_rate=0.01,\r\n",
        "                              #max_depth=4,\r\n",
        "                              #max_features='sqrt',\r\n",
        "                              #min_samples_leaf=15,\r\n",
        "                              #min_samples_split=10,\r\n",
        "                              #loss='huber',\r\n",
        "                              #random_state=42)\r\n",
        "\r\n",
        "  {'xgboost__n_estimators': [500, 1000, 1500],\r\n",
        "   'xgboost__learning_rate': [0.01, 0.03, 0.05],\r\n",
        "   'xgboost__objective': ['reg:squarederror'],\r\n",
        "   'xgboost__max_depth': [3, 5, 10],\r\n",
        "   'xgboost__gamma': [0.3, 0.6, 0.9],\r\n",
        "   'xgboost__subsample': [0.3, 0.8, 0.9],\r\n",
        "   'xgboost__colsample_bytree': [0.3, 0.8, 0.9],\r\n",
        "   'xgboost__reg_alpha': [0.00003, 0.00006, 0.00009]}\r\n",
        "\r\n",
        "  #xgboost = XGBRegressor(learning_rate=0.01,\r\n",
        "                      #n_estimators=1000,\r\n",
        "                      #max_depth=5,\r\n",
        "                      #gamma=0.6,\r\n",
        "                      #subsample=0.8,\r\n",
        "                      #colsample_bytree=0.8,\r\n",
        "                      #objective='reg:squarederror',\r\n",
        "                      #nthread=-1,\r\n",
        "                      #scale_pos_weight=1,\r\n",
        "                      #seed=27,\r\n",
        "                      #reg_alpha=0.00006,\r\n",
        "                      #random_state=42)\r\n",
        "] '''\r\n",
        "\r\n",
        "tuned_parameters = [\r\n",
        "  {'m_rf__n_estimators': [50, 200, 400],\r\n",
        "  'm_rf__max_depth': [2, 4, 7],\r\n",
        "  'm_rf__min_samples_split': [3, 5, 8],\r\n",
        "  'm_rf__max_features': [3, 5, 8],\r\n",
        "   'm_svr__epsilon': [0.004, 0.008, 0.0016],\r\n",
        "   'm_svr__C': [10, 20, 30],\r\n",
        "   'm_svr__gamma': [0.0001, 0.0003, 0.0007],\r\n",
        "   'm_bag__n_estimators': [5, 10, 15],\r\n",
        "   'm_bag__max_samples': [1, 5, 10],\r\n",
        "   'm_bag__max_features': [1, 5, 10],\r\n",
        "   'm_gbr__n_estimators': [250, 500, 750],\r\n",
        "   'm_gbr__learning_rate': [0.01, 0.03, 0.05],\r\n",
        "   'm_gbr__max_features': ['auto', 'sqrt', 'log2'],\r\n",
        "   'm_gbr__min_samples_leaf': [7, 15, 30],\r\n",
        "   'm_gbr__min_samples_split': [7, 10, 20],\r\n",
        "   'm_gbr__loss': ['huber', 'ls', 'lad'],\r\n",
        "   'xgboost__n_estimators': [500, 1000, 1500],\r\n",
        "   'xgboost__learning_rate': [0.01, 0.03, 0.05],\r\n",
        "   'xgboost__objective': ['reg:squarederror'],\r\n",
        "   'xgboost__max_depth': [3, 5, 10],\r\n",
        "   'xgboost__gamma': [0.3, 0.6, 0.9],\r\n",
        "   'xgboost__subsample': [0.3, 0.8, 0.9],\r\n",
        "   'xgboost__colsample_bytree': [0.3, 0.8, 0.9],\r\n",
        "   'xgboost__reg_alpha': [0.00003, 0.00006, 0.00009]}\r\n",
        "]\r\n",
        "\r\n",
        "#change from neg_mean_squared_log_error to mean_squared_error as Likes are already in Log scale.\r\n",
        "metric = 'neg_mean_squared_error'\r\n",
        "\r\n",
        "cv_strategy = KFold(n_splits=4, shuffle=True)\r\n",
        "grid_search = RandomizedSearchCV(model,tuned_parameters,n_iter=20,scoring=metric,cv=cv_strategy)\r\n",
        "''' grid_search = GridSearchCV(\r\n",
        "    model, tuned_parameters, scoring=metric, n_iter=10, cv=cv_strategy\r\n",
        ") '''\r\n",
        "grid_search.fit(X_train, y_train);\r\n",
        "print('Finished!')\r\n",
        "\r\n",
        "print(\"Best parameters set found on development set:\")\r\n",
        "print()\r\n",
        "print(grid_search.best_params_)\r\n",
        "\r\n",
        "print(\"Grid scores on development set:\")\r\n",
        "print()\r\n",
        "means = grid_search.cv_results_['mean_test_score']\r\n",
        "stds = grid_search.cv_results_['std_test_score']\r\n",
        "for mean, std, params in zip(means, stds, grid_search.cv_results_['params']):\r\n",
        "    print(\"%0.3f (+/-%0.03f) for %r\"\r\n",
        "          % (mean, std * 2, params))\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M9uGy_rYgwau"
      },
      "source": [
        "model.get_params()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NYK4EXSUgwav"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gkbsazgrgwav"
      },
      "source": [
        "##Model Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bz0cA_9Ugwav"
      },
      "source": [
        "#Stacking\r\n",
        "m_rf = RandomForestRegressor(n_estimators=200,\r\n",
        "                              max_depth=2,\r\n",
        "                              min_samples_split=8,\r\n",
        "                              min_samples_leaf=8,\r\n",
        "                              max_features=None,\r\n",
        "                              oob_score=True,\r\n",
        "                              random_state=42)\r\n",
        "\r\n",
        "m_svr = SVR(C=20, epsilon=0.004, gamma=0.0001)\r\n",
        "\r\n",
        "m_ab = AdaBoostRegressor(base_estimator=DecisionTreeRegressor(max_depth=5),\r\n",
        "                        n_estimators=200,\r\n",
        "                        loss='exponential',\r\n",
        "                        learning_rate=0.01,random_state=42)\r\n",
        "m_bag = BaggingRegressor(n_estimators=15,max_samples=10,max_features=5,random_state=42)\r\n",
        "\r\n",
        "xgboost = XGBRegressor(learning_rate=0.01,\r\n",
        "                      n_estimators=1000,\r\n",
        "                      max_depth=5,\r\n",
        "                      gamma=0.3,\r\n",
        "                      subsample=0.9,\r\n",
        "                      colsample_bytree=0.8,\r\n",
        "                      objective='reg:squarederror',\r\n",
        "                      nthread=-1,\r\n",
        "                      scale_pos_weight=1,\r\n",
        "                      seed=27,\r\n",
        "                      reg_alpha=0.00009,\r\n",
        "                      random_state=42)\r\n",
        "m_gbr = GradientBoostingRegressor(n_estimators=500,\r\n",
        "                              learning_rate=0.03,\r\n",
        "                              max_depth=4,\r\n",
        "                              max_features='sqrt',\r\n",
        "                              min_samples_leaf=30,\r\n",
        "                              min_samples_split=20,\r\n",
        "                              loss='ls',\r\n",
        "                              random_state=42)\r\n",
        "m_catboost = CatBoostRegressor(n_estimators=1000,\r\n",
        "                              learning_rate=0.01,\r\n",
        "                              rsm=0.4,\r\n",
        "                              random_state=42)\r\n",
        "stack_mod = StackingRegressor(\r\n",
        "      estimators=[\r\n",
        "          ('m_rf', m_rf),\r\n",
        "          # ('m_ridge', m_ridge),\r\n",
        "          ('m_svr', m_svr),\r\n",
        "          ('m_bag', m_bag),\r\n",
        "          # ('m_ab', m_ab),\r\n",
        "          ('m_gbr', m_gbr),\r\n",
        "          ('xgboost', xgboost),\r\n",
        "          # ('m_catboost', m_catboost),\r\n",
        "      ],\r\n",
        "      n_jobs=-1,\r\n",
        "      verbose=0.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gy-8Clhegwav"
      },
      "source": [
        "def run_gradient_boosting(clf, fit_params, train, target, test):\r\n",
        "  N_SPLITS = 5\r\n",
        "  oofs = np.zeros(len(train))\r\n",
        "  preds = np.zeros((len(test)))\r\n",
        "\r\n",
        "  # target = train[TARGET_COL]\r\n",
        "  features = list(train.columns.values)\r\n",
        "  folds = StratifiedKFold(n_splits = N_SPLITS,random_state=42)\r\n",
        "  stratified_target = pd.qcut(target, 10, labels = False, duplicates='drop')\r\n",
        "\r\n",
        "  feature_importances = pd.DataFrame()\r\n",
        "\r\n",
        "  for fold_, (trn_idx, val_idx) in enumerate(folds.split(train, stratified_target)):\r\n",
        "    print(f'\\n------------- Fold {fold_ + 1} -------------')\r\n",
        "\r\n",
        "    ### Training Set\r\n",
        "    X_trn, y_trn = train.iloc[trn_idx], target.iloc[trn_idx]\r\n",
        "\r\n",
        "    ### Validation Set\r\n",
        "    X_val, y_val = train.iloc[val_idx], target.iloc[val_idx]\r\n",
        "    \r\n",
        "    _ = clf.fit(X_trn, y_trn, eval_set = [(X_val, y_val)], **fit_params)\r\n",
        "    fold_importance = pd.DataFrame({'fold': fold_ + 1, 'feature': features, 'importance': clf.feature_importances_})\r\n",
        "    feature_importances = pd.concat([feature_importances, fold_importance], axis=0)\r\n",
        "\r\n",
        "    ### Instead of directly predicting the classes we will obtain the probability of positive class.\r\n",
        "    preds_val = clf.predict(X_val)\r\n",
        "    # pred_val[pred_val<0] = 0\r\n",
        "    preds_test = clf.predict(test)\r\n",
        "\r\n",
        "    fold_score = av_metric(y_val, preds_val)\r\n",
        "    print(f'\\nAV metric score for validation set is {fold_score}')\r\n",
        "\r\n",
        "    oofs[val_idx] = preds_val\r\n",
        "    preds += preds_test / N_SPLITS\r\n",
        "\r\n",
        "\r\n",
        "  oofs_score = av_metric(target, oofs)\r\n",
        "  print(f'\\n\\nAV metric for oofs is {oofs_score}')\r\n",
        "\r\n",
        "  feature_importances = feature_importances.reset_index(drop = True)\r\n",
        "  fi = feature_importances.groupby('feature')['importance'].mean().sort_values(ascending = False)[:20][::-1]\r\n",
        "  fi.plot(kind = 'barh', figsize=(12, 6))\r\n",
        "\r\n",
        "  return oofs, preds , fi\r\n",
        "\r\n",
        "def av_metric(y_true, y_pred):\r\n",
        "  return np.sqrt(mean_squared_log_error(np.exp(y_true), np.exp(y_pred)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qol6hCMDgwav"
      },
      "source": [
        "## Main code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dugr8IOygwav"
      },
      "source": [
        "### Data pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gq6pySuEgwav"
      },
      "source": [
        "tidy_df_train,tidy_df_test = tidyer(df_train,df_test)\n",
        "\n",
        "#pixel selection\n",
        "pixel_index_selection = [2043, 1947, 2046, 2142, 1893, 2139, 1986, 2238, 1944, 2079, 1950, 2337, 1848, 1752, 1509, 1797, 1983, 2181, 2040, 2082, 1413, 1989, \n",
        "                            1851, 2274, 1656, 1566, 2241, 1605, 1896, 1992, 1701, 2178, 1599, 2268, 2235, 1695, 2271, 1560, 2175, 1890, 2364, 2085, 1293, 2367, \n",
        "                            1662, 1290, 1287, 1464, 1857, 1414, 1296, 1488, 2370, 525, 1506, 1368, 2340, 1284, 1761, 1299, 1884, 1485, 1758, 1677, 1791, 1117, \n",
        "                            1680, 1302, 2334, 2145, 1305, 1779, 1389, 1212, 1308, 1584, 1773, 1776, 1114, 1392, 1788, 1899, 2160, 1770, 1860, 2157, 1881, 1206, \n",
        "                            1581, 1395, 1872, 1386, 1869, 1767, 1749, 2361, 2344, 1473, 1203]\n",
        "\n",
        "tidy_df_train = pd.concat([tidy_df_train,img_extr(img_train,pixel_index_selection)],axis=1)\n",
        "tidy_df_test = pd.concat([tidy_df_test,img_extr(img_test,pixel_index_selection)],axis=1)\n",
        "img_clust_train,img_clust_test = img_clust(img_train,img_test)\n",
        "tidy_df_train,tidy_df_test = pd.concat([tidy_df_train,img_clust_train],axis=1),pd.concat([tidy_df_test,img_clust_test],axis=1)\n",
        "\n",
        "\n",
        "num_fea = ['Name length','Profile Creation Year','Num of Followers','Num of People Following','Num of Status Updates','Num of Direct Messages','Avg Daily Profile Visit Duration in seconds',\n",
        "           'Avg Daily Profile Clicks','Img Num Black Pixels', 'Img Average Red level',\t'Img Average Green level',\t'Img Average Blue level',\n",
        "           'Sum Selected Pixels'] + ['pixel_' + str(i) for i in sorted(pixel_index_selection)]\n",
        "cat_fea = ['Cover Image Status','Profile Verification Status','Time Region','Location Public Visibility','User Language','Profile Category','Img Spectral Clusters','Img KMeans Clusters']\n",
        "prep_df_train,prep_df_test = standard_encoder(tidy_df_train,tidy_df_test,num_fea,cat_fea)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2730GvWFgwav"
      },
      "source": [
        "### Feature selection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xrOfdmGFgwav"
      },
      "source": [
        "fea_sel = feature_select(prep_df_train)\r\n",
        "all_features = fea_sel.index[1:]\r\n",
        "\r\n",
        "cols_x = all_features[0:35]   #Top correlated features. Optimal feature selection = top 35\r\n",
        "cols_y = 'Likes'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-2acgmJgwaw"
      },
      "source": [
        "### Modelization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yWkOdMEigwaw"
      },
      "source": [
        "#catbst_model = CatBoostRegressor(n_estimators = 3000,\r\n",
        "#                       learning_rate = 0.01,\r\n",
        "#                       rsm = 0.4, ## Analogous to colsample_bytree\r\n",
        "#                       random_state=2054,\r\n",
        "#                       )\r\n",
        "#fit_params = {'verbose': 200, 'early_stopping_rounds': 200}\r\n",
        "#cb_oofs,pred_cb, fi = run_gradient_boosting(catbst_model, fit_params, prep_df_train[cols_x], prep_df_train[cols_y], prep_df_test[cols_x]);\r\n",
        "\r\n",
        "stack_mod.fit(prep_df_train[cols_x],prep_df_train[cols_y]);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USuTvsVggwaw"
      },
      "source": [
        "np.sqrt(mean_squared_log_error(np.exp(prep_df_train[cols_y]), np.exp(stack_mod.predict(prep_df_train[cols_x]))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DEHlqcEUgwaw"
      },
      "source": [
        "### Submission"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "28841F5Rgwaw"
      },
      "source": [
        "import csv \r\n",
        "\r\n",
        "pred = stack_mod.predict(prep_df_test[cols_x])\r\n",
        "pred = np.abs(pred)\r\n",
        "with open('submission.csv', 'w') as csvfile:\r\n",
        "  csvwriter = csv.writer(csvfile)  \r\n",
        "  csvwriter.writerow(['Id', 'Predicted'])\r\n",
        "  for i in range(len(pred)):\r\n",
        "    csvwriter.writerow([df_test['Id'].iloc[i], np.exp(pred[i])])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wosy9mT8W2mj"
      },
      "source": [
        "# **Annex B - Neural Networks Approach (unsubmitted)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHzdTsbzW7NQ"
      },
      "source": [
        "## **Environment Set-up**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tNpSalord1qM"
      },
      "source": [
        "\n",
        "Taking care of colab disconnection :\n",
        "\n",
        "+ https://medium.com/@shivamrawat_756/how-to-prevent-google-colab-from-disconnecting-717b88a128c0"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LI0nH5Z4nG7w"
      },
      "source": [
        "cp -R /content/drive/MyDrive/02_ETUDES/01-MILA-UDEM/IFT6758/Kaggle/* ."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vryrK6S6nRED"
      },
      "source": [
        "!unzip -qq ift6758-a20.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PcyQyIADBm2P"
      },
      "source": [
        "# !pip install webcolors\n",
        "!pip install catboost\n",
        "!pip install lightgbm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jRGaiJyydqFo"
      },
      "source": [
        "!pip install scikit-image\n",
        "!pip install opencv-python\n",
        "# !git clone https://github.com/MeAmarP/sample_imgs.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-LEwNmU37XJX"
      },
      "source": [
        "## **Import Lib**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cgVh8jEonWXb"
      },
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from numpy import argmax\n",
        "from numpy import loadtxt\n",
        "\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "from keras import backend as K"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Im28fVQGcSth"
      },
      "source": [
        "import csv\n",
        "import math\n",
        "import pdb\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import mean_squared_log_error\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import (\n",
        "    RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor, \n",
        "    VotingRegressor, StackingRegressor, BaggingRegressor,\n",
        ")\n",
        "from lightgbm import LGBMRegressor\n",
        "from catboost import CatBoostRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.svm import SVR, LinearSVR\n",
        "\n",
        "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
        "from sklearn.metrics import make_scorer\n",
        "from sklearn import preprocessing\n",
        "\n",
        "\n",
        "import seaborn as sns\n",
        "from  matplotlib import pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn import linear_model\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import RobustScaler, MinMaxScaler\n",
        "from scipy.special import boxcox1p, inv_boxcox1p\n",
        "from scipy.stats import boxcox_normmax\n",
        "from scipy.stats import skew, norm\n",
        "from scipy.stats import yeojohnson\n",
        "from sklearn.preprocessing import PowerTransformer\n",
        "\n",
        "pd.set_option('display.max_columns', None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s0J21lXjieBX"
      },
      "source": [
        "# Machine Learning\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.linear_model import Perceptron\n",
        "from sklearn.svm import SVC, LinearSVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn import tree\n",
        "\n",
        "# Ensembling\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "\n",
        "# Metrics\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from keras.optimizers import SGD,Adam,RMSprop\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3UfgNh_9dyuc"
      },
      "source": [
        "#Imports\n",
        "import skimage\n",
        "import cv2 as cv2\n",
        "import sklearn\n",
        "import pandas as pd\n",
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import ipywidgets as widgets\n",
        "from ipywidgets import interact, interact_manual\n",
        "\n",
        "#Get Python and OpenCV Version\n",
        "print('OpenCV-Python Lib Version:', cv2.__version__)\n",
        "print('Python Version:',sys.version)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YuQm1i6O7s1X"
      },
      "source": [
        "##**Tabular Dataset**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_AWMjvUyjRSs"
      },
      "source": [
        "###**Data loading (and overview)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h3wR-3lD7rxY"
      },
      "source": [
        "data_train = pd.read_csv('train.csv') #converters and parse_dates could be usefull\n",
        "data_test = pd.read_csv('test.csv')\n",
        "data_train['source']='train'\n",
        "data_test['source']=\"test\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hcne6qJwfauU"
      },
      "source": [
        "def process_column_names(df):\n",
        "        return df.rename(columns=lambda x: x.replace(' ', '_').replace('?', '').lower())\n",
        "\n",
        "data_train = process_column_names(data_train)\n",
        "data_test = process_column_names(data_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cCmzLpFrGuGS"
      },
      "source": [
        "# data_submission = pd.read_csv('sample_submission.csv')\n",
        "target_colname ='num_of_profile_likes'\n",
        "data_all =pd.concat([data_train,data_test])\n",
        "print(data_train.shape)\n",
        "print(data_test.shape)\n",
        "print(data_all.shape)\n",
        "data_all.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YXX75g988tPF"
      },
      "source": [
        "data_all.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wC4eXHnl81M5"
      },
      "source": [
        "data_all.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GsTwnp6ctqrY"
      },
      "source": [
        "### **Indexing/Multi-Indexing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pLTIoEJqA4KF"
      },
      "source": [
        "print(data_all.shape)\n",
        "# test = data_all\n",
        "# Check if there is null in potential key columns\n",
        "# print(test.isnull().sum())\n",
        "# Check if there is duplicates in keys by looking at the shape\n",
        "keys = ['id', 'user_name','profile_image']\n",
        "for key in keys:\n",
        "  print(key)\n",
        "  test = data_all\n",
        "  test = test.drop_duplicates(key)\n",
        "  print(test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b1Dp39zUmsoU"
      },
      "source": [
        "keys.append('source')\n",
        "data_all = data_all.set_index(keys=keys)\n",
        "df = data_all"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eFZC_wp8pPO5"
      },
      "source": [
        "### **Types - All Features**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tFnl97RepSIu"
      },
      "source": [
        "# df.info()\n",
        "for col in df.columns:\n",
        "  print(df[col].dtypes)\n",
        "  print(df[col].unique())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acF5TNmShI5C"
      },
      "source": [
        "### **Cleaning & Uniformization - All Features**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YppQgIOkobax"
      },
      "source": [
        "df = df.replace(r'^\\s*$', np.NaN, regex=True)\n",
        "df.location_public_visibility = df.location_public_visibility.replace('??','unknown')\n",
        "df.profile_verification_status = df.profile_verification_status.replace('Pending','not verified')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "znvLJ5A0j_V6"
      },
      "source": [
        "### **Imputation - All Features**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qs0ggzfS9cX5"
      },
      "source": [
        "+ In a first time, based on the number of Null from , I will deleted some of features that I arbitrary consider informationless\n",
        "\n",
        "+ Replacing Numerical missing data by the average of the column\n",
        "\n",
        "+ I split dataset per col dtype , have to be carefull with droduplicates() since i did not create a col index as key for each subset\n",
        "\n",
        "\n",
        "+ Careful / there is something to do with location and colors\n",
        "\n",
        "+ https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.convert_dtypes.html\n",
        "+ https://machinelearningmastery.com/knn-imputation-for-missing-values-in-machine-learning/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cYcPMvIvosLa"
      },
      "source": [
        "df.profile_category = df.profile_category.fillna('unknown')\n",
        "df.profile_cover_image_status = df.profile_cover_image_status.fillna('not set')\n",
        "df.avg_daily_profile_visit_duration_in_seconds = df.avg_daily_profile_visit_duration_in_seconds.fillna(df.avg_daily_profile_visit_duration_in_seconds.median())\n",
        "df.avg_daily_profile_clicks = df.avg_daily_profile_clicks.fillna(df.avg_daily_profile_clicks.median())\n",
        "df.utc_offset = df.utc_offset.fillna(0)\n",
        "\n",
        "df.profile_text_color = df.profile_text_color.fillna('ffffff')\n",
        "df.profile_page_color = df.profile_page_color.fillna('ffffff')\n",
        "df.profile_theme_color = df.profile_theme_color.fillna('ffffff')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X0Wpv-GBn4wC"
      },
      "source": [
        "### **Conversion - All Features**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WyfiQ9_1oiK_"
      },
      "source": [
        "df.profile_creation_timestamp = pd.to_datetime(df.profile_creation_timestamp)\n",
        "col = \"profile_creation_timestamp\"\n",
        "df[str(col+\"_year\")] = df[col].dt.year\n",
        "df[str(col+\"_week\")] = df[col].dt.week\n",
        "df[str(col+\"_month\")] = df[col].dt.month\n",
        "df[str(col+\"_day\")] = df[col].dt.day\n",
        "# df[str(col+\"_day_name\")] = df[col].dt.day_name\n",
        "df[str(col+\"_week_day\")] = df[col].dt.weekday\n",
        "df[str(col+\"_hour\")] = df[col].dt.hour\n",
        "df[str(col+\"_quarter\")] = df[col].dt.quarter\n",
        "# df[str(col+\"_minute\")] = df[col].dt.minute\n",
        "# df[str(col+\"_seconde\")] = df[col].dt.second\n",
        "del(df[col])\n",
        "\n",
        "\n",
        "df.personal_url = df.personal_url.apply(lambda x:True if (pd.notnull(x)) else False)\n",
        "\n",
        "\n",
        "def hex_to_rgb(hex):\n",
        "  return tuple(int(hex[i:i+2],16) for i in (0,2,4))\n",
        "\n",
        "df.profile_text_color = df.profile_text_color.replace(\"#\",\"\").apply(lambda x: hex_to_rgb(x))\n",
        "\n",
        "suffix=\"profile_text_color\"\n",
        "col = [x+'_{}'.format(suffix) for x in [\"red\",\"green\",\"blue\"]]\n",
        "df_temp=pd.DataFrame(df[suffix].values.tolist(), columns=col, index=df.index)\n",
        "df = pd.concat([df,df_temp], axis='columns')\n",
        "del(df[suffix])\n",
        "\n",
        "df.profile_page_color = df.profile_page_color.replace(\"#\",\"\").apply(lambda x: hex_to_rgb(x))\n",
        "\n",
        "suffix=\"profile_page_color\"\n",
        "col = [x+'_{}'.format(suffix) for x in [\"red\",\"green\",\"blue\"]]\n",
        "df_temp = pd.DataFrame(df[suffix].values.tolist(), columns=col, index=df.index)\n",
        "df = pd.concat([df,df_temp], axis='columns')\n",
        "del(df[suffix])\n",
        "\n",
        "df.profile_theme_color = df.profile_theme_color.replace(\"#\",\"\").apply(lambda x: hex_to_rgb(x))\n",
        "\n",
        "suffix=\"profile_theme_color\"\n",
        "col = [x+'_{}'.format(suffix) for x in [\"red\",\"green\",\"blue\"]]\n",
        "df_temp=pd.DataFrame(df[suffix].values.tolist(), columns=col, index=df.index)\n",
        "df = pd.concat([df,df_temp], axis='columns')\n",
        "del(df[suffix])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T4sCuki4ClEx"
      },
      "source": [
        "### <font color='red'>**Features Selection - not implemented**</font>\n",
        "\n",
        "+ https://machinelearningmastery.com/feature-selection-with-real-and-categorical-data/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XVa0TB7g9MM0"
      },
      "source": [
        "# corrMatrix = df_all.loc[:, df_all.columns != target_colname].corr()\n",
        "# sns.heatmap(corrMatrix, annot=True)\n",
        "# plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a0GjKrV1-VhI"
      },
      "source": [
        "# covMatrix = np.cov(df_all.loc[:, df_all.columns != target_colname],bias=True)\n",
        "# sns.heatmap(covMatrix, annot=True, fmt='g')\n",
        "# plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cNE6UhuUnegc"
      },
      "source": [
        "# exclusion = ['location','user_time_zone','profile_text_color','profile_page_color','profile_theme_color']\n",
        "# exclusion = ['location','user_time_zone']\n",
        "# exclusion.append(\"utc_offset\")\n",
        "\n",
        "# df = df.drop(exclusion, axis=1)\n",
        "# df = df.drop_duplicates()\n",
        "\n",
        "# keep_col = [ 'profile_verification_status', 'profile_category',\n",
        "#        'num_of_followers', 'num_of_people_following', 'num_of_status_updates',\n",
        "#        'num_of_direct_messages', 'avg_daily_profile_visit_duration_in_seconds',\n",
        "#        'avg_daily_profile_clicks', 'profile_creation_timestamp_year',\n",
        "#         'num_of_profile_likes']\n",
        "\n",
        "# df = df[keep_col]\n",
        "# df = df.drop_duplicates()\n",
        "\n",
        "# # print(df.shape)\n",
        "# df.info()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWsbeeguRZGb"
      },
      "source": [
        "### <font color='black'>**Encoding - Nominal Features**</font>\n",
        "\n",
        "+ https://machinelearningmastery.com/why-one-hot-encode-data-in-machine-learning/\n",
        "+ https://machinelearningmastery.com/one-hot-encoding-for-categorical-data/\n",
        "\n",
        "+ https://stackoverflow.com/questions/40312128/how-to-lower-all-the-elements-in-a-pandas-dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GqJC9z3cqfk9"
      },
      "source": [
        "df_bool = df.select_dtypes(exclude=['float64','int64','object','datetime64[ns, UTC]'])\n",
        "df_nominal = df.select_dtypes(exclude=['float64','int64','bool','datetime64[ns, UTC]'])\n",
        "\n",
        "df_numerical = df.select_dtypes(exclude=['object','bool','datetime64[ns, UTC]'])\n",
        "df_target = df_numerical.loc[:, df_numerical.columns == target_colname]\n",
        "df_numerical = df_numerical.loc[:, df_numerical.columns != target_colname]\n",
        "\n",
        "df_datetime = df.select_dtypes(exclude=['float64','int64','object','bool'])\n",
        "df_numerical_not_scaled =  df.select_dtypes(exclude=['object','bool','datetime64[ns, UTC]'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qY_qvNZ4U02Y"
      },
      "source": [
        "# if the number of unique is to high there is probably a better way to (otherwise it can add a lot of dimensions)\n",
        "test=df_nominal\n",
        "for col in test.columns:\n",
        "  print(col)\n",
        "  print(len(test[col].unique()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JEp7p5YT9Kf7"
      },
      "source": [
        "# example of a dummy variable encoding - taking care of redonduncy\n",
        "from numpy import asarray\n",
        "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, LabelBinarizer\n",
        "\n",
        "for col in df_nominal.columns:\n",
        "  df_nominal[col] = pd.get_dummies(df_nominal[col], prefix=str(col+\"_\"))\n",
        "for col in df_bool.columns:\n",
        "  df_bool[col] = df_bool[col].replace(False,0).replace(True,1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_CiY7dbxiD8l"
      },
      "source": [
        "### <font color='red'>**Outliers - Numerical Features**</font>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0TGDU5qrDdOW"
      },
      "source": [
        "q = df_numerical[df_numerical.columns].quantile([0.25, 0.95])\n",
        "for col in df_numerical.columns:\n",
        "  print(col)\n",
        "  df_numerical[col] = df_numerical[col].clip(*q[col])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1oaBRpTP0S3H"
      },
      "source": [
        "### <font color='black'>**Scaling - Numerical Features**</font>\n",
        "\n",
        "+ https://machinelearningmastery.com/how-to-transform-target-variables-for-regression-with-scikit-learn/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eVQTxAI6Jg7B"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, PowerTransformer, Normalizer, RobustScaler\n",
        "\n",
        "\n",
        "# scaler = StandardScaler()\n",
        "scaler = MinMaxScaler()\n",
        "# scaler = PowerTransformer()\n",
        "# scaler = Normalizer()\n",
        "# scaler = RobustScaler()\n",
        "target_scaler = scaler\n",
        "\n",
        "df_numerical[df_numerical.columns] = scaler.fit_transform(df_numerical[df_numerical.columns])\n",
        "df_numerical.head()\n",
        "# df_target[df_target.columns] = target_scaler.fit_transform(df_target[df_target.columns])\n",
        "\n",
        "# scaled_data_arr = scaler.fit_transform(df_numerical)\n",
        "# df_numerical.update(pd.DataFrame(scaled_data_arr, columns=df_numerical.columns))\n",
        "# pow_transformed_data_arr = scaler.fit_transform(df_numerical.values)\n",
        "# # print(df_numerical.head())\n",
        "# # df_numerical.update(pd.DataFrame(pow_transformed_data_arr, columns=df_numerical.columns))\n",
        "# # print(df_numerical.head())\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rf4ArAcbC7MA"
      },
      "source": [
        "## **Compute PCA Principales components**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dMdFtCY8_MlT"
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "\n",
        "df_temp = df_numerical\n",
        "nb_cp = 5\n",
        "pca = PCA(n_components=nb_cp)\n",
        "col = ['cp_'+str(x) for x in range(1,nb_cp+1)]\n",
        "print(col)\n",
        "principalComponents = pca.fit_transform(df_temp.loc[:, df_temp.columns != target_colname])\n",
        "df_principal = pd.DataFrame(data = principalComponents, columns = col, index=df_temp.index)\n",
        "\n",
        "df_principal.to_csv('df_PCA_components.csv')\n",
        "df_principal.head()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "To-Om7tAGpZw"
      },
      "source": [
        "## **Compute VGG Features**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0d6c8UtN51KG"
      },
      "source": [
        "### **Loading Data (images path)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Zz1SoaVgfTr"
      },
      "source": [
        "env_path = os.path.join(os.getcwd(), 'train_profile_images/profile_images_train')\n",
        "img_list_train = os.listdir(env_path)\n",
        "img_list_train = [os.path.join(env_path,x) for x in img_list_train]\n",
        "env_path = os.path.join(os.getcwd(), 'test_profile_images/profile_images_test')\n",
        "img_list_test = os.listdir(env_path)\n",
        "img_list_test = [os.path.join(env_path,x) for x in img_list_test]\n",
        "img_list = img_list_train + img_list_test\n",
        "\n",
        "# print(img_list_train[1])\n",
        "# print(img_list_test[1])\n",
        "# print(img_list[1])\n",
        "img_test_path = os.path.join(env_path,img_list[1])\n",
        "print(img_test_path)\n",
        "img_test = cv2.imread(img_test_path,cv2.IMREAD_UNCHANGED)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xy_ZHfxr5mo9"
      },
      "source": [
        "### **Images Preparation**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xoVjkHsImHL9"
      },
      "source": [
        "\n",
        "MainImgBin = cv2.imread(img_test_path,cv2.IMREAD_UNCHANGED)\n",
        "print('DatatypeClass of Image:',type(MainImgBin))\n",
        "print('Shape/Size of Binary Img:', MainImgBin.shape)\n",
        "\n",
        "img = cv2.imread(img_test_path,cv2.IMREAD_UNCHANGED)\n",
        "print(img.shape)\n",
        "rows,cols,dim = img.shape\n",
        "\n",
        "down=left=right=top=3\n",
        "\n",
        "#Slicing using ROI\n",
        "cropped = img[top:rows-down,right:cols-left].copy()\n",
        "\n",
        "# print(cropped.shape)\n",
        "# plt.subplot(121),plt.imshow(img,cmap='gray'),plt.title('Original',color='c')\n",
        "# plt.subplot(122),plt.imshow(cropped,cmap='gray'),plt.title('Cropped',color='c')\n",
        "\n",
        "import random \n",
        "# initializing list  \n",
        "angle_list = [i for i in range(0,360,90)]\n",
        "# angle_val = random.choice(test_list) \n",
        "for angle_val in angle_list:\n",
        "  print(angle_val)\n",
        "  scaleFactor=1\n",
        "  rows,cols,dim = img.shape\n",
        "  imgCenter = (cols-1)/2.0,(rows-1)/2.0\n",
        "  #Calculate an affine matrix of 2D rotation. \n",
        "  rotateMat = cv2.getRotationMatrix2D(imgCenter,angle_val,scaleFactor)\n",
        "  # Apply an affine transformation to an image. \n",
        "  out_img = cv2.warpAffine(img,rotateMat,(cols,rows))\n",
        "  # plt.figure(figsize=(10,10))\n",
        "  # plt.subplot(1,2,1), plt.imshow(img,cmap='gray') ,plt.title('Original Image',color='c')\n",
        "  plt.subplot(1,2,2), plt.imshow(out_img,cmap='gray'), plt.title('Rotated Image',color='c')\n",
        "  plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ib4zH0bV6RB-"
      },
      "source": [
        "### **VGGs Features Extraction**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KjQZc7iV1TZk"
      },
      "source": [
        "# Using the vgg16 model as a feature extraction model\n",
        "from keras.preprocessing.image import load_img\n",
        "from keras.preprocessing.image import img_to_array\n",
        "from keras.applications.vgg16 import preprocess_input\n",
        "from keras.applications.vgg16 import decode_predictions\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.applications.vgg19 import VGG19\n",
        "\n",
        "from keras.models import Model\n",
        "from keras.layers import Dense, Conv2D, MaxPooling2D\n",
        "from keras.layers import Flatten\n",
        "from keras.models import Model\n",
        "from pickle import dump\n",
        "\n",
        "# get the input shape\n",
        "d1,d2,d3 = img_test.shape\n",
        "\n",
        "# load model\n",
        "# load model without classifier layers\n",
        "model_VGG16 = VGG16(include_top=False, input_shape = (d1, d2, d3))\n",
        "# model_VGG16 =  VGG16(weights = \"imagenet\", include_top = False, input_shape = (d1, d2, d3))\n",
        "# model_VGG19 = VGG19(weights = \"imagenet\", include_top = False, input_shape = (d1, d2, d3))\n",
        "\n",
        "\n",
        "# set the layers non trainable\n",
        "model_VGG = model_VGG16\n",
        "for layer in model_VGG.layers[:5]:\n",
        "   layer.trainable = False\n",
        "  \n",
        "# remove the output layer\n",
        "model_VGG = Model(inputs=model_VGG.inputs, outputs=model_VGG.layers[-2].output)\n",
        "# vizualize the layers\n",
        "# print(model_VGG.summary())\n",
        "\n",
        "# add new classifier layers\n",
        "flat1 = Flatten()(model_VGG.layers[-1].output)\n",
        "class1 = Dense(1024, activation='relu')(flat1)\n",
        "class2 = Dense(512, activation='relu')(class1)\n",
        "class3 = Dense(256, activation='relu')(class2)\n",
        "class4 = Dense(64, activation='relu')(class3)\n",
        "output = Dense(6, activation='softmax')(class4)\n",
        "\n",
        "# define final model\n",
        "model_final = Model(inputs=model_VGG.inputs, outputs=output)\n",
        "print(model_final.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZsRG6OpBsqr"
      },
      "source": [
        "# define cnn model\n",
        "# def define_model():\n",
        "# \tmodel_cnn_baseline = Sequential()\n",
        "# \tmodel_cnn_baseline.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', input_shape=(d1, d2, d3)))\n",
        "# \tmodel_cnn_baseline.add(MaxPooling2D((2, 2)))\n",
        "# \tmodel_cnn_baseline.add(Flatten())\n",
        "# \tmodel_cnn_baseline.add(Dense(100, activation='relu', kernel_initializer='he_uniform'))\n",
        "# \tmodel_cnn_baseline.add(Dense(10, activation='softmax'))\n",
        "# \t# compile model\n",
        "# \t# opt = Adam(lr=0.01, momentum=0.9)\n",
        "# \tmodel_cnn_baseline.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "# \treturn model_cnn_baseline\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5YC6-jtt8Om6"
      },
      "source": [
        "results = []\n",
        "col=[]\n",
        "# load an image from file\n",
        "for img_path in img_list:# for img_name in img_list[1:3]:\n",
        "\n",
        "  img_path_splited = img_path.split('/')\n",
        "  img_name = img_path_splited[int(len(img_path_splited)-1)]\n",
        "\n",
        "  # print(img_path)\n",
        "  image = load_img(img_path, target_size=(d1, d2))\n",
        "  image = img_to_array(image)\n",
        "  image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
        "  # print(image.shape)\n",
        "  # prepare the image for the VGG model\n",
        "  image = preprocess_input(image)\n",
        "  # get extracted features\n",
        "\n",
        "  features = model_final.predict(image)\n",
        "  # mod = define_model()\n",
        "  # features = mod.predict(image)\n",
        "\n",
        "  # print(features.shape)\n",
        "  \n",
        "  temp = features[0].tolist()\n",
        "  temp.append(img_name)\n",
        "  results.append(temp)\n",
        "  # save to file\n",
        "  # dump(features, open('dog.pkl', 'wb'))\n",
        "print(\"done\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dTrAE42cl619"
      },
      "source": [
        "nb_features = features.shape[1]\n",
        "col = ['feature_'+str(x) for x in range(1,nb_features+1)]\n",
        "col.append('profile_image')\n",
        "df_vgg_results = pd.DataFrame(results,columns = col)\n",
        "df_vgg_results.to_csv('df_VGG_features.csv')\n",
        "df_vgg_results.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2zDPYFdfDns"
      },
      "source": [
        "## **Merge Dataframes**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZgUIE9VCIA3n"
      },
      "source": [
        "df_all_not_scaled = pd.concat([df_nominal,df_numerical_not_scaled,df_bool], axis='columns')\n",
        "print(df_all_not_scaled.shape)\n",
        "df_all_not_scaled.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJSZ77vcrhwp"
      },
      "source": [
        "df_all_scaled = pd.concat([df_nominal,df_numerical,df_bool,df_target], axis='columns')\n",
        "print(df_all_scaled.shape)\n",
        "df_all_scaled.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2w8HA7_FfIl3"
      },
      "source": [
        "df_temp = df_all_scaled\n",
        "df_temp = df_temp.reset_index()\n",
        "df_all_VGG = df_temp.merge(df_vgg_results, left_on='profile_image', right_on='profile_image', how='left')\n",
        "df_all_VGG = df_all_VGG.set_index(keys)\n",
        "df_all_VGG.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0nZErq65q3lj"
      },
      "source": [
        "df_temp = df_all_scaled\n",
        "df_all_PCA = pd.concat([df_temp,df_principal], axis='columns')\n",
        "df_all_PCA.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m5SztRqSlxh_"
      },
      "source": [
        "print(df_all_scaled.columns)\n",
        "keep_col = ['profile_cover_image_status', 'profile_verification_status', 'location',\n",
        "        'user_language',\n",
        "       'profile_category', 'utc_offset', 'num_of_followers',\n",
        "       'num_of_people_following', 'num_of_status_updates',\n",
        "       'num_of_direct_messages', 'avg_daily_profile_visit_duration_in_seconds',\n",
        "       'avg_daily_profile_clicks', 'profile_creation_timestamp_year',\n",
        "       'profile_creation_timestamp_week', 'profile_creation_timestamp_month',\n",
        "       'profile_creation_timestamp_day', 'profile_creation_timestamp_week_day',\n",
        "       'profile_creation_timestamp_hour', 'profile_creation_timestamp_quarter',\n",
        "       'red_profile_text_color', 'green_profile_text_color',\n",
        "       'blue_profile_text_color', 'red_profile_page_color',\n",
        "       'green_profile_page_color', 'blue_profile_page_color',\n",
        "       'red_profile_theme_color', 'green_profile_theme_color',\n",
        "       'blue_profile_theme_color', 'personal_url',\n",
        "       'is_profile_view_size_customized', 'num_of_profile_likes']\n",
        "\n",
        "df_all_reduced = df_all_scaled[keep_col]\n",
        "\n",
        "df_all_reduced.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OL1Iir5KQ53f"
      },
      "source": [
        "### **Metric Definition +  Train-Test split**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S3VQX0WIQ6Ch"
      },
      "source": [
        "\n",
        "# df_all_VGG\n",
        "# df_all_PCA\n",
        "# df_all_not_scaled\n",
        "# df_all_scaled\n",
        "# df_all_reduced\n",
        "df_used = df_all_VGG\n",
        "print(df_used.shape)\n",
        "df_train= df_used[df_used.index.get_level_values('source').isin(['train'])]\n",
        "\n",
        "df_train[target_colname] = df_train[target_colname].apply(np.log1p)\n",
        "print(df_train[target_colname].describe())\n",
        "\n",
        "df_test= df_used[df_used.index.get_level_values('source').isin(['test'])]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Enypbox9HB3W"
      },
      "source": [
        "df_used.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EwmKBrzoexMK"
      },
      "source": [
        "def rmsle(y_true, y_pred):\n",
        "    return np.sqrt(mean_squared_log_error(y_true, y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vnyXYownqE1S"
      },
      "source": [
        "def root_mean_squared_error(y_true, y_pred):\n",
        "        return K.sqrt(K.mean(K.square(y_pred - y_true)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pVKPkKCsCR2Z"
      },
      "source": [
        "#### <font color='black'>*Test Target Transformation*</font>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ltXd1cyapDu"
      },
      "source": [
        "# print(df_target.describe())\n",
        "# test = pd.DataFrame(target_scaler.inverse_transform(df_target),columns=['pred'])\n",
        "# print(test.describe())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6zeulZ9jr6Hb"
      },
      "source": [
        "## **C- Neural Network Approach**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PHrpNdjpKC4t"
      },
      "source": [
        "X_train, y_train = df_train.loc[:, df_train.columns != target_colname], df_train[target_colname]\n",
        "X_test = df_test.loc[:, df_test.columns != target_colname]\n",
        "print(X_train.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jhWKPVyAjm7P"
      },
      "source": [
        "# X, y = df_train.loc[:, df_train.columns != target_colname], df_train[target_colname]\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
        "# print(X_train.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rT8M1cvBGyRT"
      },
      "source": [
        "\n",
        "results=[]\n",
        "epochs_list=[8,9,10,11,12,13,14,15]\n",
        "\n",
        "for epochs in epochs_list :\n",
        "\n",
        "  NN = Sequential()\n",
        "  NN.add(Dense(X_train.shape[1], input_dim=X_train.shape[1], kernel_initializer='normal', activation='relu'))\n",
        "  NN.add(Dense(X_train.shape[1]*3, kernel_initializer='normal', activation='relu'))\n",
        "  NN.add(Dense(X_train.shape[1]*2, kernel_initializer='normal', activation='relu'))\n",
        "  NN.add(Dense(1, kernel_initializer='normal'))\n",
        "\n",
        "  learning_rate = 0.01\n",
        "  decay_rate = learning_rate / (epochs)\n",
        "  momentum = 0.8\n",
        "  batch_size=32\n",
        "\n",
        "  opt = SGD(lr=learning_rate, momentum=momentum, decay=decay_rate, nesterov=False)\n",
        "  NN.compile(optimizer = opt, loss = 'mean_squared_error', metrics =[root_mean_squared_error])\n",
        "  \n",
        "  NN.fit(X_train, y_train, epochs=epochs, batch_size=batch_size)\n",
        "  \n",
        "  pred = NN.predict(X_test)\n",
        "  pred = np.abs(pred)\n",
        "\n",
        "  # print(rmsle(np.expm1(y_test), np.expm1(pred)))\n",
        "  results.append(rmsle(np.expm1(y_test), np.expm1(pred)))\n",
        "\n",
        "print(results)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EAcU3-WXogYY"
      },
      "source": [
        "sub_test = df_test.loc[:, df_test.columns != target_colname]\n",
        "pred = NN.predict(sub_test)\n",
        "pred_NN = pd.DataFrame(df_test.index.get_level_values('id').values, columns=[\"Id\"])\n",
        "pred_NN[\"Predicted\"] = pd.DataFrame(np.expm1(pred))\n",
        "# pred_NN[\"Predicted\"] = pd.DataFrame(target_scaler.inverse_transform(pred),columns=['pred'])\n",
        "print(pred_NN.shape)\n",
        "print(pred_NN.head())\n",
        "pred_NN.to_csv('submission_NN_last2.csv',index=False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}